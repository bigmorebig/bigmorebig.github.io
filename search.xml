<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[测试文章]]></title>
    <url>%2F2019%2F07%2F22%2F%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[从登录场景来看软件测试：1.验证功能性需求 显示功能需求：实现软件本身的功能需求 隐式功能需求：软件出现异常的处理机制是否正确 2.非功能性需求 安全：如密码加密，传输过程加密，SQL注入，XSS攻击（补充：拿到加密密码反解成本是否符合要求） 性能：响应时间是否达到要求，死锁和不合理资源等待情况，高并发情况下服务端是否存在内存泄漏 兼容性：不同浏览器，不同主机，不同网络，不同分辨率 单元测试： 第一，代码的基本特征和产生错误的原因（要做到代码功能逻辑正确，必须做到分类正确且完备没有遗漏，同事每个分类的逻辑处理正确）代码中的功能点相当于单元测试的‘等价类’ 第二，单元测试用例（单元测试是一个包含‘输入数据’和‘预计输出’的集合）输入数据并不只是被测试函数的输入参数，以下为一些分类： 1. 被测函数的输入参数 2. 被测函数内部需要读取的全局静态变量 3. 函数内部调用子函数获得的数据等等 ... 预计输出并不只是被测函数的返回值，以下为一些分类： 1. 被测函数的返回值 2. 被测函数的输出参数 3. 被测函数中进行的文件更新 4. 被测函数中进行的数据库更新等等 ... 第三，驱动代码，桩代码和Mock代码： 驱动代码指调用被测函数的代码，包含调用被测函数前的数据准备，调用被测函数和验证相关结果，驱动代码结构通常由单元测试框架决定 桩代码指用来代替函数内部某个尚未实现的函数的代码，与真是代码相比，桩代码具有与原函数相同的原形，只是内部实现不同，桩代码只是起到隔离 和补齐的作用，是被测代码能够独立编译，链接并独立运行，同时桩代码还有控制被测函数执行路径的作用，通常桩代码的验证(assert)逻辑出现在 驱动代码中 Mock代码：与桩代码类似，对于结果的验证通常出现在Mock代码中单元测试自动化的过程： 测试用例自动执行 用例框架代码生成的自动化：例如unittest框架提高开发效率 部分测试输入数据的自动化生成：例如通过条件判断语句来控制测试输入数据的自动化生成 自动桩代码的生成： 指自动化工具对被测代码进行分析扫描，自动为被测函数内部调用的其他函数生成可编程的桩代码 抽桩：在代码集成测试阶段，希望不再调用桩代码而调用真实代码 被测代码的自动化静态分析：主要指代码的静态扫描，目的是识别违反编码规则和编码风格的代码行，比较常见的工具有sonar和coverity。 测试覆盖率的自动统计与分析 集成测试：如单元测试最大的区别在于代码集成测试不允许使用桩代码web service测试 测试脚手架代码的自动化生成：例如httprunner可以自动生成项目脚手架 部分测试数据的自动化生成：与单元测试的区别在于单元测试的输入数据是函数的参数组合，API测试对应的是API的参数和API调用的payload response验证的自动化 基于测试工具(如postman)的自动化脚本生成:在使用测试工具测试时，在已经存在多个测试用例的情况下来基于代码实现API测试时，存在一个问题就在于，可以开发一个代码转换工具，自动将已有的测试用例(如JSON格式)转换为可执行代码，为后续CI/CD直接使用 测试计划： 测试范围：测什么和不测什么 测试策略：先测什么，后测什么，如何来测，采用什么样的测试类型和测试方法 测试资源：明确谁来测和在哪里测 测试进度：主要描述各类测试的开始时间，所需工作量，预计完成时间，并以此为依据来建议产品的上线发布时间 测试风险预估 需要掌握的非测试包含： 网站架构的核心知识：比如缓存，中间件，数据库等 容器技术：docker 云计算技术：配合docker使用 devops思维：以jenkins为例，到熟练应用和组合各种plugin来完成搭建灵活高效的流水线 前端开发技术：node.js pageobject模型的核心理念是：以页面(web page或native app page)为单位封装页面上的控件以及控件的部分操作，而测试用例，更确切的是说操作函数，基于页面封装对象来完成具体的操作，最典型的模式是XXXPage.YYYComponent.ZZZOperation GUI自动化的测试数据准备：对于测试的准备数据通常有三种方法：(通常最佳的选择是利用API创建数据，然后再调用数据库来达到特定的要求，实际情况可能存在混合使用) API调用：在实际项目中，通常会把这些API调用以代码的形式封装为测试数据工具 数据库操作：在数据的创建没有对外暴露接口或者创建成本过大时，可利用数据库操作来完成，由于数据库的修改可能涉及多张表这就导致了测试数据有可能会出问题，而且在实际项目中，经常出现SQL语句更新不及时而导致测试数据错误的问题，而且有可能错误还比较隐蔽，所以需要测试数据工具的版本管理 综合运用API调用和数据库操作 GUI测试数据自动生成： 根据GUI输入数据类型，以及对应的自定义的规则库自动生成测试输入数据，例如，输入框类型是string，基于规则库自动生成如NULL,SQL注入，超长字符串，数字字符等类型 对于需要组合多个测试输入数据的场景，测试数据自动生成可以完成多个测试数据的笛卡尔组合积，然后再以人工方式剔除非法的数据组合 GUI测试稳定性问题： 非预计弹出对话框：再测试过程中，弹出预期之外的对话框，处理方法为设计异常场景恢复模式 页面控件属性的细微变化：页面属性是有可能会变的，可以采用‘组合属性’来提高定位的精确度，如果在此基础上假如模糊匹配，将进一步提高控件的识别率目前QTP已实现模糊匹配，开源GUI框架还不支持，所以需要实现二次开发，思路是实现自己的对象识别控制层，也就是再原本对象识别基于上再额外封装一层，在这个额外封装层中实现模糊匹配的查询逻辑 被测系统的A/B(App/Web)测试：在测试脚本内部做分支处理，脚本能区别A/B两个版本并作出对应实现 随机的页面延迟导致控件识别失败：方法是可以假如重试机制，默认情况下是步骤级别的，也可以是页面级别或者业务级别的，这需要二次开发 测试数据问题 GUI自动化测试报告： 可以通过截图高亮显示操作步骤，对于全截图还是错误部分才截图可以设置一个autoScreenShot参数，为True时全截图，为False时部分截图，作为配置文件 相关截图函数可以在相关的Hook操作中使用或者在每个函数中使用，可根据具体业务逻辑选择 复杂场景下的API测试： 被测业务是由多个API调用协作完成：单一的前端操作可能出发多个API操作，必须从后端模拟前端接口的调用顺序；其次接口间存在数据依赖的问题后一个API根据前一个的返回来确定调用哪个API接口；此外，如何高效的获取单个前端操作所触发的API调用序列，核心思想为通过网络监控来捕捉前端操作所出发的API调用序列，可通过抓包工具或者通过大数据分析来获取序列 API测试过程的第三方依赖：API之间时存在相互依赖关系的，比如A调用B接口，但B接口还未完成，可通过Mock Server实现 异步API测试：异步API是指调用后立即返回，但实际工作并未完成，而是需要稍后查询或者回调API。对异步API的调用分两个方面，测试异步调用是否成功，和测试异步调用的业务逻辑处理是否正确。测试异步调用可通过检查返回值和后台工作线程是否被调用查看，业务逻辑部分通常需要查看数据库和消息队列中的值 自动生成API测试代码： 可以自研一个将postman输出的json文件自动转换为测试代码的工具，同时还会将测试的断言一并转换为代码 对于复杂的测试场景，比如顺序调用多个API，可以组装工具得到的测试用例代码 response结果发生变化时自动识别： 原因是因为API的向后兼容性，当向后兼容的API发生不一致的时候会发出告警 在API测试框架中引入内建数据库，推荐使用非关系型数据库(如mongodb)，然后再数据库中记录每次request和response的组合，而对于一些像时间戳的字段可以设置一个白名单过滤 微服务下的API测试： 单体架构与微服务架构：传统模式下的开发模式是单体架构，就是将所有业务场景的表示层，业务逻辑层和数据访问层都放在一个工程中，最终编译，打包，并部署再服务器上，工程大了之后容易导致难以维护且臃肿，微服务架构中一个复杂系统不再是由单体组成，可以拆分为多个服务，各个微服务运行在自己的进程中开发和部署没有依赖，甚至可以采用不同的编程语言。 解决微服务架构下的API测试：服务多了可能导致接口耦合变多，从而导致测试用例庞大，解决这个问题的核心是解决耦合性，可以使用基于消费者契约的API测试核心是只测试那些被真正实际使用到的API调用，那些没有使用到的API就不去测试。其次多服务模式下可能导致Mock Server使用泛滥，解决方法是，以之前的契约为依据，契约一般为json格式数据，我们可以以这个json文件为mock server依据使用 - Q：在基于消费者契约的API测试中，如果新开发的API或者加了新功能的API，由于之前没有实际的消费者，所欲无法通过gateway方法获得契约，应该如何操作 - A：需要两部分结合起来，老的功能走契约测试，新的功能和api继续沿用老的方法 并发数： 业务层面的并发数：指的是使用系统的用户总数 服务层面的并发数：指的是像服务器发起请求的数量 响应时间： 前端响应时间：取决于客户端收到服务端返回数据后完成渲染所消耗的时间，这与用户主观感受有关，所以不好定义 服务器响应时间：可以细化分为web服务器时间，应用服务器时间，数据库时间，以及各服务器之间通信的时间 系统吞吐量： 所有的吞吐量都必须以单位时间为前提，一般来说分为&apos;request/second&apos;,&apos;pages/second&apos;,&apos;bytes/second&apos;三种&apos;pages/second&apos;和&apos;bytes/second&apos;表示的吞吐量，主要受网络设置，服务器架构，应用服务器制约 &apos;request/second&apos;表示的吞吐量，主要受应用服务器和应用本身的限制 需要注意的是，虽说吞吐量可以反应服务器承受负载的情况，但在不同的并发用户数下，即使系统有相近的吞吐量，得到的系统性能也会不同 常用的7种性能测试： 后端性能测试：设计模式包含两种，基于性能需求目标的测试验证，和探索系统容量，并验证系统容量的可扩展性 前端性能测试：通常来说前端性能测试关注的是浏览器端的页面渲染时间，资源加载顺序，请求数量，前端缓存使用情况，资源压缩等。希望借此找到页面加载过程中比较耗时的操作和资源，然后进行针对性的优化。目前主流的是使用雅虎总结的7大类35条规则 代码级别性能测试：指在单元测试阶段就对代码的时间性能和空间性能进行评估，通常做法是对于单元测试用例执行n次，n通常是2000-5000，再取平均值，如果达到秒级，那么单元测试的实现逻辑通常需要优化 压力测试：验证系统临界饱和阶段的稳定性及性能指标 配置测试 并发测试：通常采用‘集合点并发’，比如要求的并发数为100，需要前99个用户到集合点前等待，知道最后一个用户到达时再同时发起请求，通常实际项目中，设置的并发数会比要求的并发数稍大 可靠性测试：本质是通过长时间模拟真实的系统负载来发现系统潜在的内存泄漏，链池回收等问题 性能测试的困难点： 性能需求的明确定义： 比如：医院体检需要每天支持完成8000个体检，这里需要明确是24小时还是每天工作时间，其次是由于业务原因，医院体检的高峰期时早上，所以基于2/8原则， 因该是在96分钟内完成6400人次的体检，通常会设置20%的冗余 测试结果的分析和性能问题的定位 测试数据准备的问题： On-the-fly(实时创建)模式：可以有效避免脏数据的问题，但是在微服务架构下很可能存在两个服务的不可用性，还有这种模式创建比较耗时，测试数据本身也存在复杂性问题，通常情况下，使用在易变数据的创建 Out-of-box(事先创建数据)模式：最大的问题就是脏数据的处理，有可能创建好的数据被其他人使用了而带来脏数据或者用例执行会失败，通常情况下，使用在稳定数据的创建 数据准备1.0：这种模式最典型的模式就是将索要准备的数据封装为准备函数，这种可以是基于API的，可以是基于数据库的，还可以是相结合的，这种模式带来的问题就是包含大量参数的情况下会导致组合的复杂性，解决办法是分层封装，通过调用底层接口为上层接口提供支持，但是也会带来新的问题，就是对于参数较多的情况会带来封装过于复杂的情况，还有就是底层包发生变化时，需要修改所有的调用函数，还有可能会导致数据准备函数的jar包版本升级过于频繁这也就带来的数据准备2.0 数据准备2.0：引入Builder Pattern数据准备方式，其实就是设置默认值，这样就不会带来底层发生变化上层会频繁修改的问题，其次引入Builder Strategy模式，这是为了处理以下四种情况， (1)处理已有的数据，只需搜索就可用 (2)需要创建数据 (3)有数据就使用已有的，没有就创建数据 (4)使用OUT-OF-BOX模式但是也有新问题，跨语言平台的使用 数据准备3.0：为了解决跨平台问题使用RESTFUL API接口，其次，为了自动化，引入core service和一个内部数据库，内部数据库用来存放测试用的元数据，core service在内部数据库的支持下，提供数据质量和数量的管理，原理是，当一个测试数据被创建成功后，为了使得下次调用更加高效，会自动创建一个Jenkins job，这个Jenkins job会自动创建100条同类型的数据，同时将数据保存到数据库中，当数据库中剩余的同类型数据低于20条时，对应的Jenkins job会自动补全到100条 后端高性能服务架构： 缓存：缓存可以扩展系统性能，有降低后端负载的作用，系统和软件对应不同层级的缓存 浏览器级别的缓存，会用来存储之前网络上下载的静态资源 CDN本质也是缓存，属于部署在网络服务供应商机房中的缓存 反向代理服务器也是缓存，属于用户数据中心最前端的缓存 数据库中的热点数据，在应用服务器集群中有一级缓存，在缓存服务器集群中有二级缓存 甚至在DNS服务器上也有缓存 采用缓存的原因在于二八原则，即80%的数据访问集中在20%的数据上 分布式缓存架构主流包含以下两种： 1. JBoss Cache：在所有需要缓存的机器都同步所有缓存的副本，这就导致同步的代价比较高，但是速度快 所以比较适用于规模不是很大的缓存集群 2. Memcached：集群中每个节点缓存的数据都不一样，缓存的使用者基于hash算法，所以导致速度相比较慢 但是由于缓存容量大，存储效率高而成为主流缓存的代名词对于测试人员的角度看待相关缓存测试： 1. 对于前端测试场景，需要分别考虑缓存命中和缓存不命中时的页面加载情况 2. 基于缓存过期测试策略的设计，需要考虑到必须要重新获取数据的测试场景 3. 需要针对可能存在的脏数据，进行针对性测试。缓存脏数据是指数据库中已经更新了，但是缓存中的数据还没有更新 4. 需要针对可能存在的缓存穿透进行必要的测试。缓存穿透是指访问的数据不存在，也就永远不会有被缓存的机会，会一直对数据库发起请求 5. 系统冷启动时，在缓存预热阶段的数据库访问压力是否会超过数据库实际承载能力 6. 对于分布式缓存集群的扩容来说，由于不同的缓存集群可能采取不同的算法，扩容对集群的影响也不同，所以需要进行必要的性能评估和测试网站高可用架构设计： 在硬件层面上加入必要的冗余，来保证一台或多台服务器发生故障时，网站的可用性，必要时引入集群。 对于测试人员来说，知道了应用服务器集群的工作原理，在设计测试用例时，可以针对单个节点或多个节点故障时的情况 灰度发布：前提必须保证服务器采用了集群架构，以下为假定包含100个节点的集群中升级应用版本的灰度发布 1. 首先，从负载均衡器服务器列表中删除一个节点 2. 然后，将新版本的引用部署到这台被删除节点的服务器上，并重启服务 3. 重启完成后，将这台服务器重新挂载到负载均衡服务器上，使其接受外部流量并密切观察 4. 如果没有问题将会重复步骤升级剩余节点，如果有问题会回滚上一个版本 加强应用上线前测试或开启预发布环境 网站的可伸缩性： 应用服务器的可伸缩性：对于测试人员来讲，可从以下几点入手测试角度 1. 需要通过压力测试来得出单一节点的负载承受能力 2. 验证系统的整体承受能力，是否随集群的节点数量呈线性增长 3. 集群中节点数量是否有上线 4. 新加入的节点是否可以提供与原来节点无差异的服务 5. 对于有状态的应用，是否能够实现一次会话的多次请求都被分配到一台服务器上 6. 验证负载均衡算法的准确性 缓存集群的可伸缩性：对于测试人员来讲，以下为建议点 1. 针对缓存集群中新增的节点的测试，验证其对原有缓存的影响足够小 2. 验证系统冷启动的时候，缓存中还没有任何数据时，如果此时网站负载较大，数据库是否能承受这样的压力 3. 需要验证各种情况下，缓存数据与数据库数据的一致性 4. 验证是否对已经潜在的缓存穿透攻击进行了处理 数据库的可伸缩性：对于测试人员来讲，可从以下角度考虑 1. 正确读取刚写入数据的延迟时间 ...网站的可扩展性： 消息队列：分布式消息队列采用的时生产者与消费者模式，消息的发送者就是生产者，接收者就是消费者.引入了消息队列之后，提高了系统的可扩展性。 1. 从性能上看，消息发送者不用等到接收者实际处理完才返回，也就是从原来的同步处理变成了异步处理 2. 如果消息接收者模块发生了短时间故障，此时并不影响消息发送者往消息队列中发送数据，等接收者故障恢复之后可继续处理 3. 消息队列的核心时一个无状态存储，所以当消息队列内存不够用时，可简单增加存储空间即可测试人员在测试包含消息队列情况时，需要关注的点： 1. 从构造测试数据来看，为了以解耦的方式测试系统的各个模块，需要在消息队列中构造测试数据，这也是 很多自动化测试框架会集成消息队列写入工具的主要原因 2. 还要验证消息队列的读取数据是否正确，所以自动化测试框架也会配备消息队列的读取工具 3. 还需要考虑消息队列队列满和消息队列扩容情况下的测试]]></content>
      <tags>
        <tag>Testing</tag>
        <tag>Another Tag</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hello World]]></title>
    <url>%2F2019%2F07%2F22%2Fhello-world%2F</url>
    <content type="text"><![CDATA[Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new "My New Post" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment]]></content>
  </entry>
</search>
