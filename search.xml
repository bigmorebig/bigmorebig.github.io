<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[docker应用原理]]></title>
    <url>%2F2019%2F11%2F12%2Fdocker%E5%BA%94%E7%94%A8%E5%8E%9F%E7%90%86%2F</url>
    <content type="text"><![CDATA[使用docker部署一个python应用使用到的代码如下所示，是一个典型的flask项目 1234567891011121314from flask import Flaskimport socketimport osapp = Flask(__name__)@app.route('/')def hello(): html = "&lt;h3&gt;Hello &#123;name&#125;!&lt;/h3&gt;" \ "&lt;b&gt;Hostname:&lt;/b&gt; &#123;hostname&#125;&lt;br/&gt;" return html.format(name=os.getenv("NAME", "world"), hostname=socket.gethostname()) if __name__ == "__main__": app.run(host='0.0.0.0', port=80) 同时将依赖放在同目录下的requirement.txt的目录中 12$ cat requirements.txtFlask 接下来，制作Dockerfile 1234567891011121314151617181920# 使用官方提供的Python开发镜像作为基础镜像FROM python:2.7-slim# 将工作目录切换为/appWORKDIR /app# 将当前目录下的所有内容复制到/app下ADD . /app# 使用pip命令安装这个应用所需要的依赖RUN pip install --trusted-host pypi.python.org -r requirements.txt# 允许外界访问容器的80端口EXPOSE 80# 设置环境变量ENV NAME World# 设置容器进程为：python app.py，即：这个Python应用的启动命令CMD ["python", "app.py"] 在Dockerfile中，使用一些原语来处理逻辑流程，且执行顺序是由上往下执行 值得一提的是，通常还会看到一个叫ENTRYPOINT的原语，实际上，它和CMD都是docker进程启动时所必需的的参数，完整的格式是ENTRYPOINT CMD,但是，在默认情况下，docker会提供一个默认的ENTRYPOINT参数，即/bin/bash -c，所以在这个例子中，实际运行的是/bin/bash -c python app.py 加下来，就可以制作镜像了 1$ docker build -t helloworld . -t是指加一个tag，即取一个名字，docker build会依次执行dockerfile文件，并且每一句原语会生成一层镜像。即使原语本身没有明显的修改文件操作，它回应的层也是会存在的，只不过在外界看来，这个层是空的。 exec命令进入容器内部通过docker exec命令可以进入一个正在运行的docker容器中。 实际上，Linux Namespace 创建的隔离空间虽然看不见摸不着，但一个进程的 Namespace 信息在宿主机上是确确实实存在的，并且是以一个文件的方式存在。 通过如下指令，你可以看到当前正在运行的 Docker 容器的进程号（PID）是 25686： 12$ docker inspect --format '&#123;&#123; .State.Pid &#125;&#125;' 4ddf4638572d25686 这时，你可以通过查看宿主机的 proc 文件，看到这个 25686 进程的所有 Namespace 对应的文件： 12345678910$ ls -l /proc/25686/nstotal 0lrwxrwxrwx 1 root root 0 Aug 13 14:05 cgroup -&gt; cgroup:[4026531835]lrwxrwxrwx 1 root root 0 Aug 13 14:05 ipc -&gt; ipc:[4026532278]lrwxrwxrwx 1 root root 0 Aug 13 14:05 mnt -&gt; mnt:[4026532276]lrwxrwxrwx 1 root root 0 Aug 13 14:05 net -&gt; net:[4026532281]lrwxrwxrwx 1 root root 0 Aug 13 14:05 pid -&gt; pid:[4026532279]lrwxrwxrwx 1 root root 0 Aug 13 14:05 pid_for_children -&gt; pid:[4026532279]lrwxrwxrwx 1 root root 0 Aug 13 14:05 user -&gt; user:[4026531837]lrwxrwxrwx 1 root root 0 Aug 13 14:05 uts -&gt; uts:[4026532277] 可以看到，一个进程的每种 Linux Namespace，都在它对应的 /proc/[进程号]/ns 下有一个对应的虚拟文件，并且链接到一个真实的 Namespace 文件上。这就相当于’HODL’住了所有的Namespace，那么就可以实现进入一个已经存在的Namespace中了。 这也就意味着：一个进程，可以选择加入到某个进程已有的 Namespace 当中，从而达到“进入”这个进程所在容器的目的，这正是 docker exec 的实现原理。 简单说下所依赖的原理，其实是依赖于setns()的Linux系统调用，具体用法可以百度或Google。它的作用就是加入一个进程的Namespace中。 docker commit，实际上就是在容器运行起来后，把最上层的“可读写层”，加上原先容器镜像的只读层，打包组成了一个新的镜像。当然，下面这些只读层在宿主机上是共享的，不会占用额外的空间。 由于使用了联合文件系统，容器对镜像的任何操作，都会被操作系统先复制到可读写层，然后再修改，这就是所谓的Copy On Write。当然，docker commit不会提交Init层的内容。 Volume机制volume的存在在于解决容器中文件与数据在宿主机上的备份。而volume的存在，允许将宿主机上指定的文件或目录挂载到容器中进行读取和修改。 在docker中，有两种申明方式，如下所示： 12$ docker run -v /test ...$ docker run -v /home:/test ... 这两种命令的本质是相同的，都是把一个宿主机的目录挂载到容器的/test目录中。只是在第一种情况下，由于没有申明宿主机目录，docker会默认在宿主机创建一个临时目录/var/lib/docker/volumes/[VOLUME_ID]/_data，然后将它挂载到容器的/test目录下。而第二种情况，是直接将宿主机的/home目录挂载到容器的/test目录下。 由于当容器进程被创建之后，尽管开启了 Mount Namespace，但是在它执行 chroot（或者 pivot_root）之前，容器进程一直可以看到宿主机上的整个文件系统。 而宿主机上的文件系统，也自然包括了我们要使用的容器镜像。这个镜像的各个层，保存在 /var/lib/docker/aufs/diff目录下，在容器进程启动后，它们会被联合挂载在/var/lib/docker/aufs/mnt/ 目录中，这样容器所需的 rootfs 就准备好了。 所以，我们只需要在 rootfs 准备好之后，在执行 chroot 之前，把 Volume 指定的宿主机目录（比如 /home 目录），挂载到指定的容器目录（比如 /test 目录）在宿主机上对应的目录（即 /var/lib/docker/aufs/mnt/[可读写层 ID]/test）上，这个 Volume 的挂载工作就完成了。 更重要的是，由于执行这个挂载操作时，“容器进程”已经创建了，也就意味着此时 Mount Namespace 已经开启了。所以，这个挂载事件只在这个容器里可见。你在宿主机上，是看不见容器内部的这个挂载点的。这就保证了容器的隔离性不会被 Volume 打破。 而这里要使用到的挂载技术，就是 Linux 的绑定挂载（bind mount）机制。它的主要作用就是，允许你将一个目录或者文件，而不是整个设备，挂载到一个指定的目录上。并且，这时你在该挂载点上进行的任何操作，只是发生在被挂载的目录或者文件上，而原挂载点的内容则会被隐藏起来且不受影响。 注意：这里提到的 “ 容器进程 “，是 Docker 创建的一个容器初始化进程 (dockerinit)，而不是应用进程 (ENTRYPOINT + CMD)。dockerinit 会负责完成根目录的准备、挂载设备和目录、配置 hostname 等一系列需要在容器内进行的初始化操作。最后，它通过 execv() 系统调用，让应用进程取代自己，成为容器里的 PID=1 的进程。 所以，在一个正确的时机，进行一次绑定挂载，Docker 就可以成功地将一个宿主机上的目录或文件，不动声色地挂载到容器中。 同时，/test目录中的内容，也不会被docker commit所提交。原因在于，容器的镜像操作，比如 docker commit，都是发生在宿主机空间的。而由于 Mount Namespace 的隔离作用，宿主机并不知道这个绑定挂载的存在。所以，在宿主机看来，容器中可读写层的 /test 目录（/var/lib/docker/aufs/mnt/[可读写层 ID]/test），始终是空的。 以上，就是docker volume的核心原理了。 总结一下，将rootfs分层所示如下图 参考 https://time.geekbang.org/column/article/18119 https://docs.docker.com/engine/reference/builder/]]></content>
      <categories>
        <category>容器与容器技术</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[容器技术的实现]]></title>
    <url>%2F2019%2F11%2F07%2F%E5%AE%B9%E5%99%A8%E6%8A%80%E6%9C%AF%E7%9A%84%E5%AE%9E%E7%8E%B0%2F</url>
    <content type="text"><![CDATA[容器技术的兴起 容器技术的兴起源于PaSS技术的普及 Docker公司发布的Docker项目具有里程碑式的意义 Docker项目通过“容器镜像”，解决了应用打包的根本性难题 NameSpace隔离我们经常看见一张对比虚拟机和Docker的图 这幅图的左边，画出了虚拟机的工作原理。其中，名为 Hypervisor的软件是虚拟机最主要的部分。它通过硬件虚拟化功能，模拟出了运行一个操作系统需要的各种硬件，比如 CPU、内存、I/O 设备等等。然后，它在这些虚拟的硬件上安装了一个新的操作系统，即 Guest OS。 而这幅图的右边，则用一个名为 Docker Engine 的软件替换了 Hypervisor。这也是为什么，很多人会把 Docker 项目称为“轻量级”虚拟化技术的原因，实际上就是把虚拟机的概念套在了容器上。 可是这样的说法并不严谨。容器技术的核心功能，就是通过约束和修改进程的动态表现，从而为其创造出一个“边界”。 对于 Docker 等大多数 Linux 容器来说，Cgroups 技术是用来制造约束的主要手段，而 Namespace技术则是用来修改进程视图的主要方法。 假如运行一个容器，并进入其交互界面 123$ docker run -it busybox /bin/sh/ # 再执行ps指令 12345/ # psPID USER TIME COMMAND 1 root 0:00 /bin/sh 10 root 0:00 ps 可以看见，容器中的/bin/bash的pid为1，以及命令本身包含的ps，这说明Docker已经被隔离在了一个完全隔离的世界中了。 而Docker采用的隔离机制其实就是Linux的NameSpace技术。而 Namespace 的使用方式也非常有意思：它其实只是 Linux 创建新进程的一个可选参数。我们知道，在 Linux 系统中创建线程的系统调用是 clone()，比如： 12int pid = clone(main_function, stack_size, SIGCHLD, NULL); 这个系统调用就会为我们创建一个新的进程，并且返回它的进程号 pid。而当我们用 clone() 系统调用创建一个新进程时，就可以在参数中指定 CLONE_NEWPID 参数，比如： 12int pid = clone(main_function, stack_size, CLONE_NEWPID | SIGCHLD, NULL); 这时，新创建的这个进程将会“看到”一个全新的进程空间，在这个进程空间里，它的 PID 是 1。之所以说“看到”，是因为这只是一个“障眼法”，在宿主机真实的进程空间里，这个进程的 PID 还是真实的数值，比如 100。 除了我们刚刚用到的 PID Namespace，Linux 操作系统还提供了 Mount、UTS、IPC、Network 和 User 这些 Namespace，用来对各种不同的进程上下文进行“障眼法”操作。 比如，Mount Namespace，用于让被隔离进程只看到当前 Namespace 里的挂载点信息；Network Namespace，用于让被隔离进程看到当前 Namespace 里的网络设备和配置。 这，就是 Linux 容器最基本的实现原理了。 由此可见，虚拟机和Docker的比对图应该如下所示 使用虚拟化技术作为应用沙盒，就必须要由 Hypervisor 来负责创建虚拟机，这个虚拟机是真实存在的，并且它里面必须运行一个完整的 Guest OS 才能执行用户的应用进程。这就不可避免地带来了额外的资源消耗和占用。相比之下，容器技术的应用并不需要单独的GuestOS，共享宿主机的内核。 但是，有利也有弊，Linux NameSpace在隔离上也存在隔离不彻底的问题。 首先，既然容器只是运行在宿主机上的一种特殊的进程，那么多个容器之间使用的就还是同一个宿主机的操作系统内核。 这意味着，如果你要在 Windows 宿主机上运行 Linux 容器，或者在低版本的 Linux 宿主机上运行高版本的 Linux 容器，都是行不通的。 其次，在 Linux 内核中，有很多资源和对象是不能被 Namespace 化的，最典型的例子就是：时间。 这意味着，在容器中修改了时间，宿主机的时间也会被修改，这显然不符合期望的要求。 所以总结来说，容器，就是一种特殊的进程。 Cgroups限制由于容器是一个特殊的进程，这意味着，它所能够使用到的资源（比如 CPU、内存），却是可以随时被宿主机上的其他进程（或者其他容器）占用的。当然，这个 100 号进程自己也可能把所有资源吃光。这些情况，显然都不是一个“沙盒”应该表现出来的合理行为。 好在Linux Cgroups就是Linux内核中用来设置资源限制的一个重要功能。 在 Linux 中，Cgroups 给用户暴露出来的操作接口是文件系统，即它以文件和目录的方式组织在操作系统的 /sys/fs/cgroup 路径下。可以通过mount指令显示 12345678$ mount -t cgroup cpuset on /sys/fs/cgroup/cpuset type cgroup (rw,nosuid,nodev,noexec,relatime,cpuset)cpu on /sys/fs/cgroup/cpu type cgroup (rw,nosuid,nodev,noexec,relatime,cpu)cpuacct on /sys/fs/cgroup/cpuacct type cgroup (rw,nosuid,nodev,noexec,relatime,cpuacct)blkio on /sys/fs/cgroup/blkio type cgroup (rw,nosuid,nodev,noexec,relatime,blkio)memory on /sys/fs/cgroup/memory type cgroup (rw,nosuid,nodev,noexec,relatime,memory)... 可以看到，目录下有很多诸如 cpuset、cpu、 memory 这样的子目录，也叫子系统。这些都是我这台机器当前可以被 Cgroups 进行限制的资源种类。而在子系统对应的资源种类下，你就可以看到该类资源具体可以被限制的方法。比如，对 CPU 子系统来说，我们就可以看到如下几个配置文件，这个指令是： 1234$ ls /sys/fs/cgroup/cpucgroup.clone_children cpu.cfs_period_us cpu.rt_period_us cpu.shares notify_on_releasecgroup.procs cpu.cfs_quota_us cpu.rt_runtime_us cpu.stat tasks 其中cfs_period 和 cfs_quota 这两个参数需要组合使用，可以用来限制进程在长度为 cfs_period 的一段时间内，只能被分配到总量为 cfs_quota 的 CPU 时间，tasks可以用来指定进程的PID。 对于配置文件的使用，需要到对应的子系统下创建一个目录，例如： 12345root@ubuntu:/sys/fs/cgroup/cpu$ mkdir containerroot@ubuntu:/sys/fs/cgroup/cpu$ ls container/cgroup.clone_children cpu.cfs_period_us cpu.rt_period_us cpu.shares notify_on_releasecgroup.procs cpu.cfs_quota_us cpu.rt_runtime_us cpu.stat tasks 这个目录被称为控制组，操作系统会在你新创建的 container 目录下，自动生成该子系统对应的资源限制文件。 假如在后台执行一个死循环脚本，让脚本将操作系统CPU吃满 123$ while : ; do : ; done &amp;[1] 226 可以通过top指令来确认CPU实时变化 123$ top%Cpu0 :100.0 us, 0.0 sy, 0.0 ni, 0.0 id, 0.0 wa, 0.0 hi, 0.0 si, 0.0 st 而此时，可以通过查看container目录下的文件，可以看到container 控制组里的 CPU quota 还没有任何限制（即：-1），CPU period 则是默认的 100 ms（100000 us） 12345$ cat /sys/fs/cgroup/cpu/container/cpu.cfs_quota_us -1$ cat /sys/fs/cgroup/cpu/container/cpu.cfs_period_us 100000 接下来可以通过修改配置文件的方式来对CPU占用率进行限制 1234567$ echo 20000 &gt; /sys/fs/cgroup/cpu/container/cpu.cfs_quota_us$ echo 226 &gt; /sys/fs/cgroup/cpu/container/tasks $ top%Cpu0 : 20.3 us, 0.0 sy, 0.0 ni, 79.7 id, 0.0 wa, 0.0 hi, 0.0 si, 0.0 st 可以看到，CPU占用率立马降为20%。除 CPU 子系统外，Cgroups 的每一项子系统都有其独有的资源限制能力，比如： blkio：为块设备设定I/O 限制，一般用于磁盘等设备； cpuset：为进程分配单独的 CPU 核和对应的内存节点； memory：为进程设定内存使用的限制。 Linux Cgroups 的设计还是比较易用的，简单粗暴地理解呢，它就是一个子系统目录加上一组资源限制文件的组合。当然，至于控制组下面的资源文件限制值，可以在运行image的时候通过参数来指定 123456$ docker run -it --cpu-period=100000 --cpu-quota=20000 ubuntu /bin/bash$ cat /sys/fs/cgroup/cpu/docker/5d5c9f67d/cpu.cfs_period_us 100000$ cat /sys/fs/cgroup/cpu/docker/5d5c9f67d/cpu.cfs_quota_us 20000 总结来说，容器是一个单进程模型，这意味着，一个容器没法运行两个不同的应用，因为没法找到一个公共的PID=1来充当两个不同应用的父进程。这是因为容器本身的设计就是希望应用和容器同生命周期。 同样，Cgroups也是有利有弊，最明显的就是/proc文件系统问题。 /proc是用来存放当前内核的运行状态的一些特殊文件集合，比如CPU使用情况，内存使用情况，top指令的主要来源值就是来自于这里。但是，在容器中，proc显示的是宿主机的数据，这是绝对不允许的。 当然，可采用lxcfs来隔离宿主机和容器的文件系统。 容器文件系统的深入理解由于NameSpace的作用是隔离，它的应用让容器只能看到该NameSpace内的世界；Cgroups的作用是限制，，它的作用是限制容器对于资源的使用率。对于理想的容器文件系统应该是看到一份完全独立的文件系统，且不受宿主机的影响。关于Mount NameSpace的小实验可以参考DOCKER基础技术,可以得出结论是： Mount Namespace 跟其他 Namespace 的使用略有不同的地方：它对容器进程视图的改变，一定是伴随着挂载操作（mount）才能生效。 对于刚进入容器的文件系统来说，容器使用的是Linux名为chroot的指令，即改变进程根目录到指定的目录下。用法如下所示： 首先，创建一个test目录和几个lib文件夹，再把bash命令拷贝到test目录下对应的bin目录下，接下来，把 bash 命令需要的所有 so 文件，也拷贝到 test 目录对应的 lib 路径下。找到 so 文件可以用 ldd 命令 12345678$ mkdir -p $HOME/test$ mkdir -p $HOME/test/&#123;bin,lib64,lib&#125;$ cd $T$ cp -v /bin/&#123;bash,ls&#125; $HOME/test/bin$ T=$HOME/test$ list="$(ldd /bin/ls | egrep -o '/lib.*\.[0-9]')"$ for i in $list; do cp -v "$i" "$&#123;T&#125;$&#123;i&#125;"; done 最后，执行 chroot 命令，告诉操作系统，我们将使用 $HOME/test 目录作为 /bin/bash 进程的根目录 1$ chroot $HOME/test /bin/bash 这时，你如果执行 “ls /“，就会看到，它返回的都是 $HOME/test 目录下面的内容，而不是宿主机的内容。实际上，Mount Namespace 正是基于对 chroot 的不断改良才被发明出来的，它也是 Linux 操作系统里的第一个 Namespace。 当然，为了能够让容器的这个根目录看起来更“真实”，我们一般会在这个容器的根目录下挂载一个完整操作系统的文件系统，比如 Ubuntu16.04 的 ISO。 而这个挂载在容器根目录上、用来为容器进程提供隔离后执行环境的文件系统，就是所谓的“容器镜像”。它还有一个更为专业的名字，叫作：rootfs（根文件系统）。 现在，对于Docker项目来说，最核心的原理已经完成了，即： 启用 Linux Namespace 配置； 设置指定的 Cgroups 参数； 切换进程的根目录（Change Root）。 不过需要注意的是，rootfs 只是一个操作系统所包含的文件、配置和目录，并不包括操作系统内核。在 Linux 操作系统中，这两部分是分开存放的，操作系统只有在开机启动时才会加载指定版本的内核镜像。 也就是说，同一台机器上的所有容器，都共享一个操作系统内核，这也是容器相比于虚拟机的主要区别：毕竟后者有模拟出来的硬件机器充当沙盒，每个沙盒中还有一个完整的GuestOS。 当然，在有了rootfs之后，还必须面对的一个问题就是，是否需要每次改动一下应用都需要重新制作一次rootfs。 Docker的做法是，在镜像的设计中，引入了层（layer）的概念。也就是说，用户制作镜像的每一步操作，都会生成一个层，也就是一个增量 rootfs。 这个做法主要依赖Linux的联合文件系统(Union File System)，最主要的功能是将多个不同位置的目录联合挂载（union mount）到同一个目录下。比如，我现在有两个目录 A 和 B，它们分别有两个文件： 12345678$ tree.├── A│ ├── a│ └── x└── B ├── b └── x 然后，我使用联合挂载的方式，将这两个目录挂载到一个公共的目录 C 上： 12$ mkdir C$ mount -t aufs -o dirs=./A:./B none ./C 这时，我再查看目录 C 的内容，就能看到目录 A 和 B 下的文件被合并到了一起： 12345$ tree ./C./C├── a├── b└── x Docker采用AuFS这个联合文件系统来实现联合挂载，对于 AuFS 来说，它最关键的目录结构在 /var/lib/docker 路径下的 diff 目录：/var/lib/docker/aufs/diff/&lt;layer_id&gt;，而在使用镜像时，Docker 会把这些增量联合挂载在一个统一的挂载点上/var/lib/docker/aufs/mnt/&lt;layer_id&gt;。 而且，从这个结构可以看出来，这个容器的 rootfs 由如下图所示的三部分组成： 第一部分：只读层 它是这个容器的 rootfs 最下面的五层，对应的正是 ubuntu:latest 镜像的五层。 123456$ ls /var/lib/docker/aufs/diff/72b0744e06247c7d0...etc sbin usr var$ ls /var/lib/docker/aufs/diff/32e8e20064858c0f2...run$ ls /var/lib/docker/aufs/diff/a524a729adadedb900...bin boot dev etc home lib lib64 media mnt opt proc root run sbin srv sys tmp usr var 可以看到，这些层，都以增量的方式分别包含了 Ubuntu 操作系统的一部分。 第二部分：可读写层 在没有写入文件之前，这个目录是空的。而一旦在容器里做了写操作，你修改产生的内容就会以增量的方式出现在这个层中。 而对于删除来说，AuFS 会在可读写层创建一个 whiteout 文件，把只读层里的文件“遮挡”起来。比如，你要删除只读层里一个名叫 foo 的文件，那么这个删除操作实际上是在可读写层创建了一个名叫.wh.foo 的文件。这样，当这两个层被联合挂载之后，foo 文件就会被.wh.foo 文件“遮挡”起来，“消失”了。 第三部分：Init层 它是一个以-init结尾的层，夹在只读层和读写层之间。Init 层是 Docker 项目单独生成的一个内部层，专门用来存放 /etc/hosts、/etc/resolv.conf 等信息。 需要这样一层的原因是，这些文件本来属于只读的 Ubuntu 镜像的一部分，但是用户往往需要在启动容器时写入一些指定的值比如 hostname，所以就需要在可读写层对它们进行修改。 所以，Docker 做法是，在修改了这些文件之后，以一个单独的层挂载了出来。而用户执行 docker commit 只会提交可读写层，所以是不包含这些内容的。 参考 https://coolshell.cn/articles/17010.html https://time.geekbang.org/column/intro/116]]></content>
      <categories>
        <category>容器与容器技术</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Python的垃圾回收机制]]></title>
    <url>%2F2019%2F08%2F16%2F%E5%BC%95%E7%94%A8%E8%AE%A1%E6%95%B0%2F</url>
    <content type="text"><![CDATA[引用计数引用计数是指当对象的引用计数(指针数)为0时，表示这个对象不可达，需要被回收 12345678910111213141516171819202122232425import osimport psutil# 显示当前 python 程序占用的内存大小def show_memory_info(hint): pid = os.getpid() p = psutil.Process(pid) info = p.memory_full_info() memory = info.uss / 1024. / 1024 print('&#123;&#125; memory used: &#123;&#125; MB'.format(hint, memory))def func(): show_memory_info('initial') a = [i for i in range(10000000)] show_memory_info('after a created')func()show_memory_info('finished')########## 输出 ##########initial memory used: 47.19140625 MBafter a created memory used: 433.91015625 MBfinished memory used: 48.109375 MB 这个例子说明的是，a是局部变量，当包含a的函数执行完成之后，a的引用就变为0，也就被回收了，想要改变这种方式的话，只需要将a设置为global变量即可 这里存在的问题是，如果a是被返回的值，那么列表a的生命周期就没有消失，仍然会继续占用内存 12345def func(): show_memory_info('initial') a = [i for i in range(10000000)] show_memory_info('after a created') return a 查看对象的引用次数可用sys模块中的getrefcount函数 12345a = []print(sys.getrefcount(a)) #output:2(一次来自于a，一次来自于getrefcount自身)def get_count(a): print(sys.getrefcount(a))#output:4(a,python函数调用，函数参数，getrefcount) 需要注意的是，函数调用时，函数本身和函数参数都会产生调用，sys.getrefcount()并不是统计的指针数，而是统计指针指向的变量数量 123456789101112&gt;&gt;&gt; a = []&gt;&gt;&gt; import sys&gt;&gt;&gt; print(sys.getrefcount(a))2&gt;&gt;&gt; b = a&gt;&gt;&gt; print(sys.getrefcount(a))3&gt;&gt;&gt; c = b&gt;&gt;&gt; d = b&gt;&gt;&gt; f = c&gt;&gt;&gt; print(sys.getrefcount(a))6 而回收内存的方法也很简单，先调用del语句，在强制调用gc.collect()手动启动垃圾回收即可 循环引用python中绝大部分都采用的引用计数的垃圾回收机制，但是，当多个计数存在相互调用的情况下时，可能为内存带来很大的负担 12345678910111213141516def func(): show_memory_info('initial') a = [i for i in range(10000000)] b = [i for i in range(10000000)] show_memory_info('after a, b created') a.append(b) b.append(a)func()show_memory_info('finished')#output"""initial memory used: 8.2421875 MBafter a, b created memory used: 783.05078125 MBfinished memory used: 783.05078125 MB""" 由上例可以看出，当func函数执行完之后，按理说a, b的生命周期已经结束，但是可以看出内存并没有释放。因为他们还有相互引用，导致内存并没有释放。 如果想要回收内存，可以显示调用gc.collect()手动启动垃圾回收 123func()gc.collect()show_memory_info('finished') python采用标记清除算法和分代收集，来启用针对循环引用的自动垃圾回收。 标记清除大致的意思是，python会想一个有向图一样去遍历每个节点，如果存在没有被标记的节点就会被回收，当然，不可能每次都去遍历全部节点，python采用的是双向链表维护了一个数据结构。 分代收集是指，python将所有对象分为三代。刚刚创建的对象为第0代，经过一次垃圾回收之后，依然存在的对象便会从上一代挪到下一代。而每一代都会有自动垃圾回收的阈值，值是可以单独指定的，当垃圾回收时，到达阈值时，对象就会被回收。]]></content>
      <categories>
        <category>python</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[深入理解迭代器和生成器]]></title>
    <url>%2F2019%2F08%2F09%2F%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3%E8%BF%AD%E4%BB%A3%E5%99%A8%E5%92%8C%E7%94%9F%E6%88%90%E5%99%A8%2F</url>
    <content type="text"><![CDATA[对于生成器来说，最大的作用可能是对于内存的节省，因为每次调用next()函数才会去取一次数据，直到抛出StopIteration异常。但这并不是本次笔记的重点，本次笔记的重点在于一定要牢记每次调用生成器之后，都会消耗生成器中的值 12345&gt;&gt;&gt; a = (i for i in range(3))&gt;&gt;&gt; next(a)0&gt;&gt;&gt; list(a)[1, 2] 对于经常使用的(i in a)来说，其实它底层调用的还是生成器原理 1234while True: val = next(a) if val == i: yield True 所以，先看以下示例 1234567891011b = (i for i in range(5))print(2 in b)print(4 in b)print(3 in b)########## 输出 ##########TrueTrueFalse 原因在于每次使用print函数时，都会消耗掉b中的值，而每次使用in函数时，比如2 in b就会消耗掉0, 1, 2这三个值，剩下3, 4这两个值，而4 in b时，其中存在4，就返回为True，此时b中已消耗完所有元素，在此调用就会失败。 在看一个leetcode示例，给定字符串 s 和 t ，判断 s 是否为 t 的子序列。 1234567891011def is_subsequence(a, b): b = iter(b) return all(i in b for i in a)print(is_subsequence([1, 3, 5], [1, 2, 3, 4, 5]))print(is_subsequence([1, 4, 3], [1, 2, 3, 4, 5]))########## 输出 ##########TrueFalse]]></content>
      <categories>
        <category>python</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Python的参数传递机制]]></title>
    <url>%2F2019%2F08%2F09%2Fpython%E5%8F%82%E6%95%B0%E4%BC%A0%E9%80%92%2F</url>
    <content type="text"><![CDATA[首先看以下示例： 12345678&gt;&gt;&gt; a = 1&gt;&gt;&gt; b = 1&gt;&gt;&gt; a is bTrue&gt;&gt;&gt; a = [1, 2]&gt;&gt;&gt; b = [1, 2]&gt;&gt;&gt; a is bFalse 变量的赋值，表示让变量指向某个对象，并不是拷贝对象给变量；而一个对象是可以由多个对象指向 可变对象(列表，字典，集合等)的改变，会影响所有指向该对象的变量 对于不可变对象(字符串，整型，元祖)的改变，所有指向该对象的变量的值总是一样的，也不会改变。但是通过某些操作(+,=等等)更新不可变对象时，会返回一个新的对象 变量可以被删除，但对象无法被删除。这也和python的垃圾回收机制相符合，只有当该对象的指向变量为0个时，才会回收该对象。 通过以下示例： 1234567&gt;&gt;&gt; a = 1&gt;&gt;&gt; b = a&gt;&gt;&gt; a += 1&gt;&gt;&gt; a2&gt;&gt;&gt; b1 由于1是不可变对象，a, b都是指向1这个变量，a的值变化时，相当于a的指向又会重新改变，而b的指向是没有变化的 而对于可变对象来说，a, b会同时指向一个内存地址，改变可变对象的值，会改变所有指向该对象的变量 1234567&gt;&gt;&gt; a = [1, 2]&gt;&gt;&gt; b = a&gt;&gt;&gt; a.append(3)&gt;&gt;&gt; a[1, 2, 3]&gt;&gt;&gt; b[1, 2, 3] 1234567&gt;&gt;&gt; def func(a):... a = 2...&gt;&gt;&gt; b = 1&gt;&gt;&gt; func(b)&gt;&gt;&gt; b1 在上述例子中，变量a和b同时指向1这个对象，当执行到a = 2时，是将a重新指向了2这个对象，而b仍然还是指向的1这个对象 如果想要改变上述过程 1234567&gt;&gt;&gt; def func(a):... a = 2... return a&gt;&gt;&gt; b = 1&gt;&gt;&gt; b = func(b)&gt;&gt;&gt; b2 可将b变量重新指向2这个对象，这样就能改变b的值 而对于可变参数来说，改变对象的值，会改变所有指向它的值 123456&gt;&gt;&gt; def func(a):... a.append(4)&gt;&gt;&gt; b = [1, 2, 3]&gt;&gt;&gt; func(b)&gt;&gt;&gt; b[1, 2, 3, 4] 1234567891011121314&gt;&gt;&gt; def func(a):... a += [4]...&gt;&gt;&gt; b = [1, 2, 3]&gt;&gt;&gt; func(b)&gt;&gt;&gt; b[1, 2, 3, 4]&gt;&gt;&gt; def func(a):... a = a + [4]...&gt;&gt;&gt; b = [1, 2, 3]&gt;&gt;&gt; func(b)&gt;&gt;&gt; b[1, 2, 3] 需要注意的是，a += [4]和a = a + [4]是不相同的操作，a = a + [4]是表示新创建一个新的列表，然后让a重新指向它，而a += [4]是一个自增的过程 123456789&gt;&gt;&gt; a = [1]&gt;&gt;&gt; id(a)2610677016776&gt;&gt;&gt; a += [2]&gt;&gt;&gt; id(a)2610677016776&gt;&gt;&gt; a = a + [3]&gt;&gt;&gt; id(a)2610680008648 总结来说： 如果对象是可变的，对象改变时，指向这个对象的所有变量都会改变 如果对象是不可变的，对象改变时，指向这个对象的变量不会受到影响 为了安全，函数末尾尽量使用return返回]]></content>
      <categories>
        <category>python</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[装饰器]]></title>
    <url>%2F2019%2F08%2F09%2F%E8%A3%85%E9%A5%B0%E5%99%A8%2F</url>
    <content type="text"><![CDATA[之前对于装饰器的理解感觉比较片面，没成体系，在此记录一下装饰器的学习之路 Decorators is to modify the behavior of the function through a wrapper so we don’t have to actually modify the function. 也就是说，所谓装饰器，就是通过装饰器函数，来修改原函数的一些功能，使原函数在不需要修改的情况下达到某些目的。通常广泛应用于日志，身份认证，缓存等方面。 函数装饰器先看一个简单例子 1234567891011def my_decorator(func): def wrapper(): print('wrapper of decorator') func() return wrapper@my_decoratordef greet(): print('hello world')greet() @被称为语法糖，@my_decorator就相当于my_decorator(greet)，当然，my_decorator和greet函数也能带参数 12345def my_decorator(func): def wrapper(*args, **kwargs): print('wrapper of decorator') func(*args, **kwargs) return wrapper 这样也不是万无一失的，我们发现greet函数的元信息发生了变化 123456789greet.__name__## 输出'wrapper'help(greet)# 输出Help on function wrapper in module __main__:wrapper(*args, **kwargs) 为了解决这个问题，通常使用内置装饰器@functools.wrap，它会帮助我们保留函数的元信息 1234567891011121314151617import functoolsdef my_decorator(func): @functools.wraps(func) def wrapper(*args, **kwargs): print('wrapper of decorator') func(*args, **kwargs) return wrapper @my_decoratordef greet(message): print(message)greet.__name__# 输出'greet' 类装饰器不仅不可以使用函数装饰器，类也可以使用装饰器。类装饰器主要依赖于__call__函数 123456789101112131415161718192021222324252627class Count: def __init__(self, func): self.func = func self.num_calls = 0 def __call__(self, *args, **kwargs): self.num_calls += 1 print('num of calls is: &#123;&#125;'.format(self.num_calls)) return self.func(*args, **kwargs)@Countdef example(): print("hello world")example()# 输出num of calls is: 1hello worldexample()# 输出num of calls is: 2hello world... 装饰器也可以是嵌套使用 12345@decorator1@decorator2@decorator3def func(): ... 相当于 1decorator1(decorator2(decorator3(func))) 实际用法最常用的应该还是日志记录。如果想要测试某些函数的耗时时长，装饰器就是一种常用方式 12345678910111213141516import timeimport functoolsdef log_execution_time(func): @functools.wraps(func) def wrapper(*args, **kwargs): start = time.perf_counter() res = func(*args, **kwargs) end = time.perf_counter() print('&#123;&#125; took &#123;&#125; ms'.format(func.__name__, (end - start) * 1000)) return res return wrapper @log_execution_timedef calculate_similarity(items): ... 对于身份认证来说，往往使用需要登录之后才能使用的功能，例如评论，在校验这种身份的时候也经常使用装饰器来校验 123456789101112131415import functoolsdef authenticate(func): @functools.wraps(func) def wrapper(*args, **kwargs): request = args[0] if check_user_logged_in(request): # 如果用户处于登录状态 return func(*args, **kwargs) # 执行函数 post_comment() else: raise Exception('Authentication failed') return wrapper @authenticatedef post_comment(request, ...) ... 另外，对于测试来说，往往需要传入某种格式固定的参数，例如传入值的先后顺序时候合法等，也常用装饰器来进行校验 12345678910import functoolsdef validation_check(input): @functools.wraps(func) def wrapper(*args, **kwargs): ... # 检查输入是否合法 @validation_checkdef neural_network_training(param1, param2, ...): ...]]></content>
      <categories>
        <category>python</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Python比较符与拷贝问题]]></title>
    <url>%2F2019%2F08%2F07%2Fpython%E6%AF%94%E8%BE%83%E4%B8%8E%E6%8B%B7%E8%B4%9D%E9%97%AE%E9%A2%98%2F</url>
    <content type="text"><![CDATA[is VS ==简单来说，==比较的是值大小，is比较的是内存地址，总的来说，is运算符的速度大于==运算符，这是在于is运算符没办法重载，而运行==运算符时，程序会先去搜索__eq__函数，如果没有重载，就会直接比较大小，由于python很多内置的函数都重载了__eq__函数 值得注意的是，对于整型数字来说，a is b为True的结论，只适用于-5到256之间，比如： 12345678&gt;&gt;&gt; a = 257&gt;&gt;&gt; b = 257&gt;&gt;&gt; id(a)2314450519952&gt;&gt;&gt; id(b)2314450941424&gt;&gt;&gt; a is bFalse 原因在于，出于性能考虑，python内部会维持一个-5到256的数组，起到一个缓存的作用。每次你去创建一个-5到256范围的整型数字时，python会去这个数组中取值，而不是去创建一个新的内存。 浅拷贝与深拷贝浅拷贝：是指重新分配一块内存，创建一个新的对象，里面的元素是对原对象中子对象的引用。如果原对象中元素是不可变类型，不会产生影响，如果是可变类型，会带来新对象的改变。 深拷贝：指重新分配一块新的内存地址，创建一个新的对象和新的元素，和原对象没有任何关联。 注意，浅拷贝带来的变化只针对原对象的子对象是可变类型数据，例如嵌套列表 12345678&gt;&gt;&gt; a = [[1, 2], 3]&gt;&gt;&gt; b = a[:]&gt;&gt;&gt; a[0].append(4)&gt;&gt;&gt; b[[1, 2, 4], 3]&gt;&gt;&gt; a.append(5)&gt;&gt;&gt; b[[1, 2, 4], 3] 12345678910&gt;&gt;&gt; a = [[1, 2], (3, 4)]&gt;&gt;&gt; a[1] += (5, )&gt;&gt;&gt; a[[1, 2], (3, 4, 5)]&gt;&gt;&gt; b = a[:]&gt;&gt;&gt; a[1] += (5, )&gt;&gt;&gt; b[[1, 2], (3, 4, 5)]&gt;&gt;&gt; a[[1, 2], (3, 4, 5, 5)] 而深拷贝不会受到任何影响 1234567&gt;&gt;&gt; from copy import deepcopy&gt;&gt;&gt; a[[1, 2], (3, 4, 5, 5)]&gt;&gt;&gt; b = deepcopy(a)&gt;&gt;&gt; a[0].append(3)&gt;&gt;&gt; b[[1, 2], (3, 4, 5, 5)] 深拷贝也不是完美的，如果被拷贝对象是自身，程序有可能会陷入无限循环 1234567&gt;&gt;&gt; x = [1]&gt;&gt;&gt; x.append(x)&gt;&gt;&gt; x[1, [...]]&gt;&gt;&gt; y = deepcopy(x)&gt;&gt;&gt; y[1, [...]] x为一个无限嵌套的列表，深拷贝的时候，程序并没有报stack overflow的现象，这是在于深拷贝函数内部维护这一个字典，记录已经拷贝对象和ID，拷贝过程中，如果字典里存储了将要拷贝对象，会直接将字典中内容返回 而在使用==比较深拷贝之后的无限嵌套列表时，会抛出异常RecursionError: maximum recursion depth exceeded in comparison，这是在于列表中==会去遍历每个值的大小再去比较，而列表时无限嵌套的，所以会抛出异常 12345import copyx = [1]x.append(x)y = copy.deepcopy(x)print(x == y) #RecursionError: maximum recursion depth exceeded in comparison]]></content>
      <categories>
        <category>python</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[sql范式]]></title>
    <url>%2F2019%2F08%2F07%2FSQL%E8%BF%9B%E9%98%B6%2F</url>
    <content type="text"><![CDATA[数据库范式我们在设计数据库模型的时候，需要对关系内部各个属性之间联系的合理化程度进行定义，这就有了不同等级的规范要求，这些规范要求就被称为范式(NF)。也可以理解为，一张数据表的设计结构需要满足的某种设计标准级别。 目前关系型数据库一共有6种范式，，由低到高分别为： 1NF（第一范式） 2NF（第二范式） 3NF（第三范式） BCNF（巴斯-科德范式） 4NF（第四范式） 5NF（第五范式，也叫完美范式） 数据库范式的设计越高阶，冗余度就越低，同时高阶的范式一定会满足低阶的要求。 数据库表中键的定义： 超键：能唯一标识的属性集 候选键：如果超键中不包含多余的属性，那么这个键就是超键 主键：用户可以从候选键中选择一个键作为主键 外键：如果R1数据表中某属性集不是R1的主键，而是另一个表R2的主键，那么这个键就是R1的主键 主属性：包含任一候选键的属性成为主属性 非主属性：与主属性相对，不包含任何一个候选键的属性 第一范式：指的是数据表中任何属性都是原子性的，不可再分。几乎所有关系型数据库都满足第一范式的要求。 第二范式：指的是数据表的非主属性都要和这个数据表的候选键有完全依赖。以球员表player_game表为例，包含球员编号，姓名，年龄，比赛编号，比赛时间和比赛场地等属性，这里的候选键和主键分别为(球员编号和比赛编号)。但是这个数据表设计并不满足第二范式，因为还存在以下对应关系: 12(球员编号)--&gt; (姓名，年龄)(比赛编号) --&gt; (比赛时间，比赛场地) 这样会产生以下问题： 数据冗余：如果一个球员可以参加n场比赛，那么球员的姓名和年龄就重复了n-1次。一个比赛可能有m个球员参加，比赛时间和场地就重复了m-1次 插入异常：我们想要添加一场新的比赛，但是这时还没确定参加的球员都有谁，那么久没法插入 删除异常：比如想要删除某个球员编号，如果没有单独保存比赛表时，就会把比赛信息也删除掉 更新异常：如果想要调整某个比赛时间，那么数据表中所有时间都要调整 为避免以上情况，可以将一张表拆分为3张表。球员player表包含球员编号，年龄，姓名；比赛game表包含比赛编号，比赛时间和比赛场地等属性；球员关系player_game表包含球员编号，比赛编号，比赛得分等属性 第三范式：对任何非主属性都不传递依赖于候选键。 以球员player表为例，这张表包含球员编号，姓名，球队名称，球队主教练。球员编号决定了球队名称，球队名称决定了球队主教练，那么非主属性球队主教练就依赖于候选键球员编号。需要将player表拆分为下面这样： 球员表包含球员编号，姓名和球队名称；球队表包含球队名称和球队主教练。 当然，也不一定是范式越高就越好，越高阶以为这冗余越少，同时数据表也越多，搜索的时间也越大。实际工作中往往根据实际情况适当采用反范式，以时间换取空间的做法，容忍适当的冗余。 仓库名 管理员 物品名 数量 北京仓 张三 iPhone XR 10 北京仓 张三 iPhone 7 20 上海仓 李四 iPhone 8 30 上海仓 李四 iPhone X 40 在上表中，一个仓库只有一个管理员，同时一个管理员也管理者一个仓库。这样候选键为(仓库名，物品名)和(管理员，物品名)，然后我们从中选取一个候选键作为主键。按照以上理论梳理，此表满足了1NF,2NF,3NF规范，但是同样存在以下问题： 增加一个仓库，但是没有存放任何物品，根据数据完整性的要求，主键不能有空值，因此会出现插入异常 仓库管理员更换之后，会修改多条记录 仓库物品卖空后，仓库名称和管理员都会随之删除掉 由此引入BCNF(巴斯-科德范式)：它在3NF的基础上消除了主属性对候选键的部分依赖或者传递依赖关系 按照BCNF要求，我们需要将上表拆分为两个表： 仓库表：(仓库名，管理员) 库存表：（仓库名，物品名，数量）]]></content>
      <categories>
        <category>sql</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[sql基础]]></title>
    <url>%2F2019%2F08%2F05%2FSQL%2F</url>
    <content type="text"><![CDATA[Oracle执行流程 语法检查：检查 SQL 拼写是否正确，如果不正确，Oracle 会报语法错误。 语义检查：检查 SQL 中的访问对象是否存在。比如我们在写SELECT 语句的时候，列名写错了，系统就会提示错误。语法检查和语义检查的作用是保证 SQL 语句没有错误。 权限检查：看用户是否具备访问该数据的权限。 共享池检查：共享池(Shared Pool)是一块内存池，最主要的作用是缓存SQL语句和该语句的执行计划。Oracle通过检查共享池是否存在SQL语句的执行计划，来判断进行软解析 软解析：在共享池中，Oracle首先对SQL语句进行Hash运算，然后根据Hash值在库缓存(Library Cache)中查找，如果存在SQL语句的执行计划，就直接拿来用，直接进入’执行器’环节 硬解析：如果没找到SQL语句和执行计划，Oracle就需要创建解析树进行解析，生成执行计划，进入’优化器’这个环节 优化器：优化器中就需要进行硬解析，也就是决定怎么做，比如创建执行树，生成执行计划 执行器：执行SQL语句 MYSQL架构 MYSQL是典型的的C/S架构，服务端程序应用的是mysqld 连接层：客户端和服务器建立连接，客户端发送SQL至服务端 SQL层：对SQL语句进行查询处理 存储引擎层：与数据文件打交道，负责数据存储和读取 MYSQL执行流程 缓存查询：Server在缓存中查询到了这条语句，会直接将结果返回给客户端，如果没有，就会进入解析器阶段。需要说明的是，因为查询效率不高，所以在MySQL8.0之后就抛弃掉这个功能 解析器：在解析器中对SQL语句进行语法分析，语句分析 优化器：在优化器中会确定SQL语句的执行路径 执行器：在执行之前需要判断用户是否具备权限，如果具备权限就执行SQL查询并返回结果。在MySQL8.0以下版本，会将查询结果缓存。 SELECT执行顺序 查询的关键字顺序 1SELECT ... FROM ... WHERE ... GROUP BY ... HAVING ... ORDER BY ... SELECT的执行顺序(在MYSQL和Oracle中SELECT的执行顺序基本相同) 1FROM &gt; WHERE &gt; GROUP BY &gt; HAVING &gt; SELECT 的字段 &gt; DISTINCT &gt; ORDER BY &gt; LIMIT 详细顺序如下所示 1234567SELECT DISTINCT player_id, player_name, count(*) as num # 顺序 5FROM player JOIN team ON player.team_id = team.team_id # 顺序 1WHERE height &gt; 1.80 # 顺序 2GROUP BY player.team_id # 顺序 3HAVING num &gt; 2 # 顺序 4ORDER BY num DESC # 顺序 6LIMIT 2 # 顺序 7 SQL内置函数通常将内置函数分为四类： 算术函数 字符串函数 日期函数 转换函数 算术函数 函数名 定义 ABS() 去绝对值 MOD() 取余 ROUND() 四舍五入为指定小数位数，需要有两个参数，分别为字段名称和小数位数 123SELECT ABS(-2); #output:2SELECT MOD(10, 3); #output:1SELECT ROUND(1.123456, 2) #output:1.12 字符串函数 函数名 定义 CONCAT() 将多个字段拼接起来 LENGTH() 计算字段长度，一个汉字算3个字符 CHAR_LENGTH() 计算字段长度，汉字，字母，数字都算1个字符 LOWER() 将字符串转换为小写 UPPER() 将字符串转换为大写 REPLACE() 替换函数，有3个参数，分别为：要替换的表达式或字段名，old,new SUBSTRING() 截取字符串，有3个参数，分别为：字符串，开始位置，截取长度（下边从1开始） 12345678-- SELECT CONCAT('aa','bb','cc') as concat;-- SELECT LENGTH('qwerty') as len;-- SELECT LENGTH('唐肖') as han_len;-- SELECT CHAR_LENGTH('唐肖') as char_len;-- SELECT LOWER('PYTHON') as low;-- SELECT UPPER('python') as up;-- SELECT REPLACE('weekday', 'k','ken') as re;SELECT SUBSTRING('python', 1, 3) as sub; 日期函数 函数名 定义 CURRENT_DATE() 系统当前日期 CURRENT_TIME() 系统当前时间，没有具体日期 CURRENT_TIMESTAMP() 系统当前时间，包含日期和时间 EXTRACT() 抽取具体的年，月，日 DATE() 返回时间的日期部分 YEAR() 返回时间的年 MONTH() 返回时间的月 DAY() 返回时间的天数 HOUR() 返回时间的小时 MINUTE() 返回时间的分钟 SECOND() 返回时间的秒 转换函数 函数名 定义 CAST() 数据类型转换，参数是一个表达式，表达式通过AS关键词分割了2个参数，分别是原始数据和目标类型数据 COALESCE() 返回第一个非空数值 子查询 参数 含义 EXISTS 判断条件是否存在，存在未True，否则为False IN 判断是否在结果集中 ANY 需要与比较操作符一起使用，与子查询返回的任何值做比较 ALL 需要与比较操作符一起使用，与子查询返回的所有值做比较 查看出场过的球员都有哪些 1SELECT player_id, team_id, player_name FROM player WHERE player_id in (SELECT player_id FROM player_score WHERE player.player_id = player_score.player_id) 查询球员表中，比印第安纳步行者（对应的 team_id 为1002）中任何一个球员身高高的球员的信息 1SQL: SELECT player_id, player_name, height FROM player WHERE height &gt; ANY (SELECT height FROM player WHERE team_id = 1002) 查询球员表中，比印第安纳步行者（对应的 team_id 为1002）中所有球员身高高的球员的信息 1SQL: SELECT player_id, player_name, height FROM player WHERE height &gt; ALL (SELECT height FROM player WHERE team_id = 1002) 查询场均得分大于 20 的球员。场均得分从player_score 表中获取 1SELECT player_id,team_id,player_name FROM player WHERE player_id IN(SELECT player_id FROM player_score GROUP BY player_id HAVING AVG(score) &gt; 20); 联表查询由于主流关系型数据库对SQL92的支持更好，在此以SQL92作为示例。 交叉查询1SELECT * FROM player CROSS JOIN team 通过CROSS JOIN关键字可以得到两张表的笛卡尔积查询结果，当然也可以多次使用该关键字来连接多张表 自然连接使用NATURAL JOIN关键字可以自动连接两张表相同的关键字，然后进行等值连接 1SELECT player_id, team_id, player_name, height, team_name FROM player NATURAL JOIN team ON连接ON用来指定连接条件，ON player.team_id = team.team_id相当于是指定team_id字段的等值连接 1SELECT player_id, player.team_id, player_name, height, team_name FROM player JOIN team ON player.team_id = team.team_id 当然，也可以进行非等值连接 123SELECT p.player_name, p.height, h.height_levelFROM player as p JOIN height_grades as hON height BETWEEN h.height_lowest AND h.height_highest USING使用USING关键字指定数据库中相同字段名进行等值连接 1SELECT player_id, team_id, player_name, height, team_name FROM player JOIN team USING(team_id) 外连接包含三种连接方式： 左连接：LEFT JOIN 右连接：RIGHT JOIN 全连接：FULL JOIN 值得注意的是，MYSQL并不支持全连接 例子：根据不同身高等级查询球员的个数，输出身高等级和个数 1SELECT height_level, count(*) FROM height_grades as h JOIN player as p ON p.height BETWEEN h.height_lowest AND h.height_highest GROUP BY height_level; 视图视图作为一张虚拟表，只是帮助我们封装底层与数据库接口，相当于一张表或多张表的结果集，视图一般在数据量比较大的情况下使用，它具有以下特点， 安全性：虚拟表是基于底层数据库的，我们在使用视图时一般不会通过视图对底层数据进行修改，在一定程度上保证了数据的安全性，同时，还可以针对不同用户开放不同的数据查询权限 简单清晰：视图是对SQL语句的封装，将原来复杂的语句简单化，类似于函数的作用 创建视图 1234CREATE VIEW view_name ASSELECT column1, column2FROM tableWHERE condition 例： 1CREATE VIEW avg_height AS SELECT AVG(height) FROM player; 同时，视图还支持视图嵌套 修改视图 1234ALTER VIEW view_name ASSELECT column1, column2FROM tableWHERE condition 删除视图 1DROP VIEW view_name 关于视图的应用 查询球员中的身高介于1.90m到2.08m之间的名字，身高，以及对应的身高等级 创建身高等级的视图 1234CREATE VIEW player_height_grades ASSELECT p.player_name, p.height, h.height_levelFROM player as p JOIN height_grades as hON height BETWEEN h.height_lowest AND h.height_highest 查询身高介于1.90m到2.08m之间的名字，身高，以及对应的身高等级 1SELECT * FROM player_height_grades WHERE height &gt;= 1.90 AND height &lt;= 2.08 事务事务的特性(ACID)是：要么全部成功，要么全部失败。这保证了数据的一致性和可恢复性，它保证了我们在增加，删除，修改的时候某一个环节出错也能回滚还原。 Oracle是支持事务的，在MYSQL中，InnoDB才支持事务，可以通过SHOW ENGINES查看哪些存储引擎支持事务 事务的流程控制语句： START TRANSACTION或BEGIN：显式开启一个事务 COMMIT：提交事务。提交事务之后，对数据库的修改是永久性的。 ROLLBACK或ROLLBACK TO [SAVEPOINT]：回滚事务，表示撤销当前所有没有提交的修改或回滚到某个保存点 SAVEPOINT：在事务中创建保存点，一个事务可以有多个保存点 RELEASE SAVEPOINT：删除某个保存点 SET TRANSACTION：设置事务的隔离级别 需要注意的是，使用事务有两种，分别为隐式事务和显式事务。隐式事务实际上就是自动提交，Oracle不自动提交，需要手写COMMIT命令，而MYSQL自动提交 12mysql&gt; set autocommit =0; // 关闭自动提交mysql&gt; set autocommit =1; // 开启自动提交 在MYSQL默认情况下 123456789CREATE TABLE test(name varchar(255), PRIMARY KEY (name)) ENGINE=InnoDB;BEGIN;INSERT INTO test SELECT '关羽';COMMIT;BEGIN;INSERT INTO test SELECT '张飞';INSERT INTO test SELECT '张飞';ROLLBACK;SELECT * FROM test; 表中存在一条数据关羽，原因在于name为主键，插入第二条数据的name字段为张飞时，抛出异常，回滚到上一次事务提交点 MYSQL中completion_type参数： 1SET @@completion_type = 1; 123456789CREATE TABLE test(name varchar(255), PRIMARY KEY (name)) ENGINE=InnoDB;SET @@completion_type = 1;BEGIN;INSERT INTO test SELECT '关羽';COMMIT;INSERT INTO test SELECT '张飞';INSERT INTO test SELECT '张飞';ROLLBACK;SELECT * FROM test; 表中存在一条数据，原因在于completion=1相当于在提交之后，在下一行写下BEGIN completion=0,默认情况，在我们执行COMMIT的时候提交事务，在执行下一个事务时，还需要START TRANSACTION或BEGIN来开启 completion=1，提交事务之后，相当于是执行了COMMIT AND CHAIN，也就是开启了一个链式事务，即当我们提交了事务之后会开启一个相同隔离级别的事务 completion=2，这种情况下COMMIT=COMMIT AND RELEASE，在我们提交之后会自动断开与服务器连接 事务隔离级别严格来讲，我们可以使用串行化的方式来执行每个事务，这就意味着每个事务相互独立，不存在并发的情况。在生产中，往往存在高并发情况，这时需要降低数据库的隔离标准来换取事务之间的并发数 三种异常情况 脏读(DIRTY READ)：读到了其他用户还没提交的事务 不可重复读(Nnrepeatable Read)：对某数据进行读取，发现两次结果不同。这时由于有其他事务对这个数据进行了修改 幻读(Phantom Read)：事务A根据条件查询到了N条事务，但此时B事务更改了符合事务A查询条件的数据，事务A再次查询发现数据不一致 针对不同的异常情况，SQL92设置了4中隔离级别 脏读 不可重复读 幻读 读未提交(READ UNCOMMITTED) 允许 允许 允许 读已提交(READ COMMITTED) 禁止 允许 允许 可重复读(REPEATABLE READ) 禁止 禁止 允许 可串行化(SERIALIZABLE) 禁止 禁止 禁止 读已提交属于RDBMS中常见的默认隔离级别(比如Oracle和SQL Server)，如果想要避免不可重复读和幻读，需要在SQL查询时编写带锁的SQL语句 可重复读，是MYSQL默认的隔离级别 PYTHON操作MYSQL接口在此使用pymysql模块来操作mysql接口 connection可以对当前数据库的连接进行管理，它提供以下接口 指定host，user，passwd，port，database等参数连接数据库 db.cursor()：创建游标 db.close():关闭连接 db.begin()：开启事务 db.commit()和db.rollback()：事务提交和回滚 游标提供的接口： cursor.execute():执行sql语句 cursor.fetchone()：读取查询结果一条数据 cursor.fetchall()：读取查询结果全部数据，以元祖类型返回 cursor.fetchmany(n)：读取查询结果多条数据，以元祖类型返回 cursor.rowcount：返回查询的行数 cursor.close()：关闭游标。 为了保证数据修改的正确，可用try..except…模式捕获异常 123456789101112131415import tracebacktry: sql = "INSERT INTO player (team_id, player_name, height) VALUES (%s, %s, %s)" val = (1003, " 约翰 - 科林斯 ", 2.08) cursor.execute(sql, val) db.commit() print(cursor.rowcount, " 记录插入成功。")except Exception as e: # 打印异常信息 traceback.print_exc() # 回滚 db.rollback()finally: # 关闭数据库连接 db.close() Python ORM框架操作MYSQLORM(Object Relation Mapping),使用ORM框架的原因在于随着项目的增加，降低维护成本，且不用关系底层的SQL语句是如何写的，就可以像类或者函数一样使用。以下示例都基于sqlalchemy 初始化表结构12345678910111213141516171819from sqlalchemy import Column, String, create_engine, Integer, Floatfrom sqlalchemy.orm import sessionmakerfrom sqlalchemy.ext.declarative import declarative_basefrom sqlalchemy import func# 创建对象的基类:Base = declarative_base()# 定义User对象:class Player(Base):# 表的名字: __tablename__ = 'player' # 表的结构: player_id = Column(Integer, primary_key=True, autoincrement=True) team_id = Column(Integer) player_name = Column(String(255)) height = Column(Float(3, 2))# 初始化数据库连接:engine = create_engine('mysql+pymysql://root:123456@localhost:3306/heros_data')# 创建DBSession类型:DBSession = sessionmaker(bind=engine)session = DBSession() 连接格式为数据库类型+数据库连接框架://用户名：密码@host:port/数据库名 __tablename指明了对应的数据库表名称 在 SQLAlchemy 中，我们采用 Column 对字段进行定义，常用的数据类型如下： Integer 整数型 Float 浮点型 Decimal 定点类型 Boolean 布尔类型 Date datetime.date日期类型 Time datetime.date时间类型 String 字符类型，使用时需指明长度 Text 文本类型 除了数据类型之外，也可以指定Column参数 default 默认值 primary_key 是否为主键 unique 是否唯一 autoincrement 是否自增 增加数据1234# 新增一行数据new_player = Player(team_id=1002, player_name='唐潇唐', height=1.71)session.add(new_player)session.commit() 修改数据12345# 将球员身高为2.08的全部改为2.09rows = session.query(Player).filter(Player.height == 2.08).all()for i in rows: i.height = 2.09session.commit()session.close() 删除数据1234row = session.query(Player).filter(Player.player_name=='约翰 - 科林斯').first()session.delete(row)session.commit()session.close() 查询数据query(Player)相当于是select *,这时可以对player表中所有字段进行打印 filter()函数相当于是WHERE条件查询 多条件查询时，比如查询身高大于等于2.08，小于等于2.10的球员 1rows = session.query(Player).filter(Player.height &gt;=2.08, Player.height &lt;=2.10).all() 使用or查询时，需要引入or_方法 12from sqlalchemy import or_rows = session.query(Player).filter(or_(Player.height &gt;=2.08, Player.height &lt;=2.10)).all() 分组查询，排序，统计等需要引入func方法 123from sqlalchemy import funcrows = session.query(Player.team_id, func.count(Player.player_id)).group_by(Player.team_id).having(func.count(Player.player_id)&gt;5).order_by(func.count(Player.player_id).asc()).all()print(rows) 关于条件查询更多的接口，可以查看filter方法和func方法的源码目录]]></content>
      <categories>
        <category>sql</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[shell文本处理三剑客之sed]]></title>
    <url>%2F2019%2F08%2F01%2Fshell%E6%96%87%E6%9C%AC%E5%A4%84%E7%90%86%E4%B8%89%E5%89%91%E5%AE%A2%E4%B9%8Bsed%2F</url>
    <content type="text"><![CDATA[sed 是 stream editor 的缩写，中文称之为“流编辑器”。 sed 命令是一个面向行处理的工具，它以“行”为处理单位，针对每一行进行处理，处理后的结果会输出到标准输出（STDOUT）。你会发现 sed 命令是很懂礼貌的一个命令，它不会对读取的文件做任何贸然的修改，而是将内容都输出到标准输出中。 格式1sed [option] '[pattern]/command' filename 除此之外，sed也支持管道符来传递引用文本 option参数 参数 含义 -e 多段编辑 -n 只打印出查询到的值 -f 可将某个文件作为patter传入 -r 支持扩展正则表达式 -i 修改源文件的内容 patter参数 参数 含义 10command 匹配第10行 10,20command 匹配从第10行到第20行 10，+5command 匹配从第10行开始，到第16行结束 /patter/command 匹配含有patter的行 /patter1/,/patter2/command 匹配到从含有patter1开始，到patter2结束的所有行 10,/patter/command 匹配从第10行开始，到含有patter的行结束的所有行 /patter/,10command 匹配从含有patter的行开始，到第10行结束的所有行 command参数 参数 含义 p 打印 a 行后增加 i 行前增加 r 外部文件读入，行后增加 w 将匹配到的行写入外部文件 d 删除 s/old/new 将行内的第1个old替换为new s/old/new/g 将行内的所有old替换为new s/old/new/2g 将行内的从第2处开始到结束的old替换为new s/old/new/ig 将行内的所有old替换为new，且忽略大小写 反向引用反向引用的意思是将前面匹配到的值通过参数的形式传递给后一个，可用&amp;或\1传递，例如： 12sed -i 's/\(redis_Port=\).*/\15555/g' tmp.cfg #修改redis的端口号为5555#result：redis_Port=5555 值得注意的是，\1支持使用括号来指定反向引用的内容，而&amp;并不支持这个功能 查询 参数 含义 1p 打印第1行的内容 1,10p 打印第1行到第10行的内容 1，+5p 打印第1行到第6行的内容 /patter/p 打印含有patter的行 /patter1/, /patter2/p 打印从含有patter1的行开始，到含有patter2的行结束 /patter/, 10p 打印从含有patter的行开始，到第10行结束 10，/patter/p 打印从第10行开始，到含有patter的行结束 123456# 查询包含redis的所有行[srvc@vm-kvm5645-app ~]$ sed -n '/redis/p' tmp.cfgredis_Port=5555redis_Timeout=1redis_PORT_ALARM=6379redis_TIMEOUT_ALARM=5 修改 参数 含义 1s/old/new/ 替换第一行的old为new 1,10s/old/new/ 将1到10行中的old替换为new 1, +5s/old/new/ 将1到6行中的old替换为new /patter/s/old/new/ 将含有patter的行中的old替换为new /patter1/,/patter2/s/old/new/ 将从含有patter1开始的行到patter2结束行中的old替换为new /patter/,10s/old/new/ 将从patter开始的行到第10行结束的old替换为new 10,/patter/s/old/new/ 将从第10行开始到含有patter行结束的old替换为new 12# 将redis的超时时间修改为3ssed -i '/\[.*\]/,/\[.*\]/s/\(redis_Timeout=\).*/\13/g' tmp.cfg 删除 参数 含义 1d 删除第1行的内容 1,10d 删除第1行到第10行的内容 1,+5d 删除第1行到第6行内容 /patter/d 删除含有patter的行 /patter1/,/patter2/d 删除从包含patter1的行开始，到包含patter2结束的所有行 /patter/, 10d 删除从包含patter的行开始，到第10行结束的所有行 10,/patter/d 删除从第10行开始，到含有patter的行结束的所有行 12# 删除配置文件中的注释行和空行sed -i '/^#/d;/^$/d' tmp.cfg shell脚本对于自动化测试来说，shell脚本的存在价值似乎并没有太大必要，而且由于公司使用的是python来作为自动化编程语言，问题在于，使用RF执行python脚本修改服务器上配置文件时，老是会出错，且通过beyond compares对比，修改之后有问题的文件和之前正确的文件无任何差别，初步判断有可能是编码问题，由于公司的RF必须使用jybot来执行脚本，于是，为了不纠结这个问题，使用shell脚本处理 123456789101112[srvc@vm-kvm5645-app auto_test]$ more modify_wf.sh#/bin/bash##author:tangxiao#modify redis password or host in WEATHER.cfgfunction modify&#123;sed -i "/\#weather_open/,/redis_Timeout/ s/$1/$2/g" /opt/app/todewf/bin/WEATHER.cfg&#125;modify $1 $2 需要注意的点是，由于引用了变量$1和$2，patter部分语句必须使用双引号，且执行脚本的时候，传入的参数必须使用引号 1sh modify_wf.sh '\(redis_Pass=\).*' '\1$&#123;redis_pass&#125;' 参考： sed基本语法 shell正则表达式基本用法]]></content>
      <categories>
        <category>shell</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[关于不同测试阶段的测试场景]]></title>
    <url>%2F2019%2F07%2F31%2F%E5%85%B3%E4%BA%8E%E4%B8%8D%E5%90%8C%E6%B5%8B%E8%AF%95%E9%98%B6%E6%AE%B5%E6%B5%8B%E8%AF%95%E5%9C%BA%E6%99%AF%2F</url>
    <content type="text"><![CDATA[单元测试代码的基本特征和产生错误的原因（要做到代码功能逻辑正确，必须做到分类正确且完备没有遗漏，同事每个分类的逻辑处理正确）代码中的功能点相当于单元测试的等价类 单元测试用例（单元测试是一个包含输入数据和预计输出的集合）输入数据并不只是被测试函数的输入参数，以下为一些分类： 12341. 被测函数的输入参数 2. 被测函数内部需要读取的全局静态变量3. 函数内部调用子函数获得的数据等等... 预计输出并不只是被测函数的返回值，以下为一些分类： 123451. 被测函数的返回值2. 被测函数的输出参数3. 被测函数中进行的文件更新4. 被测函数中进行的数据库更新等等... 驱动代码，桩代码和Mock代码驱动代码: 调用被测函数的代码，包含调用被测函数前的数据准备，调用被测函数和验证相关结果，驱动代码结构通常由单元测试框架决定。 桩代码: 指用来代替函数内部某个尚未实现的函数的代码，与真是代码相比，桩代码具有与原函数相同的原形，只是内部实现不同，桩代码只是起到隔离和补齐的作用，是被测代码能够独立编译，链接并独立运行，同时桩代码还有控制被测函数执行路径的作用，通常桩代码的验证(assert)逻辑出现在驱动代码中 Mock代码: 与桩代码类似，对于结果的验证通常出现在Mock代码中 单元测试自动化的过程 测试用例自动执行 用例框架代码生成的自动化,如unittest框架提高开发效率 部分测试输入数据的自动化生成,如通过条件判断语句来控制测试输入数据的自动化生成 自动桩代码的生成 指自动化工具对被测代码进行分析扫描，自 动为被测函数内部调用的其他函数生成可编程的桩代码 抽桩：在代码集成测试阶段，希望不再调用桩代码而调用真实代码 被测代码的自动化静态分析：主要指代码的静态扫描，目的是识别违反编码规则和编码风格的代码行，比较常见的工具有sonar和coverity。 测试覆盖率的自动统计与分析 集成测试如单元测试最大的区别在于代码集成测试不允许使用桩代码web service测试 测试脚手架代码的自动化生成,如httprunner可以自动生成项目脚手架 部分测试数据的自动化生成,与单元测试的区别在于单元测试的输入数据是函数的参数组合，API测试对应的是API的参数和API调用的payload response验证的自动化 基于测试工具(如postman)的自动化脚本生成:在使用测试工具测试时，在已经存在多个测试用例的情况下来基于代码实现API测试时，存在一个问题就在于，可以开发一个代码转换工具，自动将已有的测试用例(如JSON格式)转换为可执行代码，为后续CI/CD直接使用 功能测试与性能测试验证功能性需求 显示功能需求：实现软件本身的功能需求 隐式功能需求：软件出现异常的处理机制是否正确 非功能性需求 安全：如密码加密，传输过程加密，SQL注入，XSS攻击（补充：拿到加密密码反解成本是否符合要求） 性能：响应时间是否达到要求，死锁和不合理资源等待情况，高并发情况下服务端是否存在内存泄漏 兼容性：不同浏览器，不同主机，不同网络，不同分辨率]]></content>
      <categories>
        <category>测试</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[处理xml文件]]></title>
    <url>%2F2019%2F07%2F31%2F%E5%A4%84%E7%90%86xml%E6%96%87%E4%BB%B6%2F</url>
    <content type="text"><![CDATA[python处理xml文件通常存在多种方式，这里分别以处理简单文件和大文件为例 处理一般文件处理一般文件，通常使用ElementTree模块,python3.3之后会自动寻找可用的C库来加快速度 1234try: import xml.etree.cElementTree as ETexcept ImportError: import xml.etree.ElementTree as ET 查询xml解析根节点** 123tree = ET.parse('111.xml')root = tree.getroot()#&lt;Element data at 0x1f74dabef48&gt;, so root is data 而获取root的原因在于方便后面解析使用, 通常情况，xml结构标识为&lt;tag attrib1=1&gt;text&lt;/tag&gt;tail 1234567#source data:&lt;country name="Liechtenstein"&gt;&gt;&gt;&gt; root[0].attrib #获取属性&#123;'name': 'Liechtenstein'&#125;&gt;&gt;&gt; root[0][0].text #获取文本2&gt;&gt;&gt; root[0][0].tag #获取标签rank 当然，root也是可以迭代的 12for i in root: print(i.tag, i.attrib, i.text, i.tail, sep=';', end='') 也可以是根据某一个标签进行迭代 12345678for i in root.findall('country'): #遍历所有符合条件子节点...for i in root.find('country'): #遍历第一个符合条件子节点...for i in root.findtext('country'): #只遍历文本...for i in root.iter('country'): #以当前节点为树节点... 修改xml需要注意的是，xml中所有字符均为字符串类型，需要注意字符转换 12345rank.text = str(new_rank) # 必须将int转为strrank.set("updated", "yes") # 添加属性del rank.attrib['updated'] #删除属性tree.write('111.xml') #将数据写入磁盘root.remove(country) #删除子节点 修改之后的内容只是放在内存中，所以需要将内存里面的数据保存到磁盘中 值得注意的是，python为查询的接口提供了find和findall接口，分别表示查询第一个值就返回和返回所有查询到的值，这就不要注意find并不支持xpath路径查找，如果想要使用xpath查找要使用findall 12for i in root.findall('country[2]/year'): print(i.text) 除此之外，还能对元素使用索引和切片，比如： 123i = root.findall('country')print(i[1:2])#output:[&lt;Element 'country' at 0x000002A4D5794138&gt;] 处理大型xml文件 当然，处理大型文档，除了使用固有的函数模块之外，还可以使用普通文档解析方式，这样只不过会导致取值更麻烦而已 其实只要一想到处理大型数据，就应该第一时间想到迭代器或者生成器 1234567891011121314151617181920212223from xml.etree.ElementTree import iterparsedef parse_and_remove(filename, path): path_parts = path.split('/') doc = iterparse(filename, ('start', 'end')) # Skip the root element next(doc) tag_stack = [] elem_stack = [] for event, elem in doc: if event == 'start': tag_stack.append(elem.tag) elem_stack.append(elem) elif event == 'end': if tag_stack == path_parts: yield elem elem_stack[-2].remove(elem) try: tag_stack.pop() elem_stack.pop() except IndexError: pass iterparse() 方法允许对XML文档进行增量操作。 使用时，你需要提供文件名和一个包含下面一种或多种类型的事件列表： start, end, start-ns 和 end-ns 。由 iterparse() 创建的迭代器会产生形如 (event, elem) 的元组， 其中 event 是上述事件列表中的某一个，而 elem 是相应的XML元素。 start 事件在某个元素第一次被创建并且还没有被插入其他数据(如子元素)时被创建。 而 end 事件在某个元素已经完成时被创建。 在 yield 之后的下面这个语句才是使得程序占用极少内存的ElementTree的核心特性： 1elem_stack[-2].remove(elem) 这个语句使得之前由 yield 产生的元素从它的父节点中删除掉。 假设已经没有其它的地方引用这个元素了，那么这个元素就被销毁并回收内存。 对节点的迭代式解析和删除的最终效果就是一个在文档上高效的增量式清扫过程。 文档树结构从始自终没被完整的创建过。尽管如此，还是能通过上述简单的方式来处理这个XML数据。 将字典类型数据转换为xml存在两种解决方案： 手动构造，以字符串的format函数替代的方式来构造，不过这样显得有点蠢 使用xml.etree.ElementTree模块中的Element函数 123456789101112from xml.etree.ElementTree import Element, tostringdefdict_to_xml(tag, d): elem = Element(tag) for key, val in d.items(): child = Element(key) child.text = str(val) elem.append(child) return elems = &#123;'name': 'GOOG', 'shares': 100, 'price': 490.1&#125;e = dict_to_xml('stock', s) #&lt;Element 'stock' at 0x000001CE0548C908&gt;print(tostring(e).decode('utf-8'))#&lt;stock&gt;&lt;name&gt;GOOG&lt;/name&gt;&lt;shares&gt;100&lt;/shares&gt;&lt;price&gt;490.1&lt;/price&gt;&lt;/stock&gt; 这样做的目的在于，可以通过查询数据库中的值放进字典中，利用字典生成xml文件]]></content>
      <categories>
        <category>python</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Linux之uniq]]></title>
    <url>%2F2019%2F07%2F28%2Flinux%E4%B9%8Buniq%2F</url>
    <content type="text"><![CDATA[作用 Linux uniq 命令用于检查及删除文本文件中重复出现的行列，一般与 sort 命令结合使用。 uniq 可检查文本文件中重复出现的行列。参数 -c 在每列旁边显示该行重复出现的次数。 -d 仅显示重复出现的行列。 -f&lt;栏位&gt; 忽略比较指定的栏位。 -s&lt;字符位置&gt; 忽略比较指定的字符。 -u 仅显示出一次的行列。 -w[字符位置] 指定要比较的字符。 –help 显示帮助。 –version 显示版本信息。 [输入文件] 指定已排序好的文本文件。如果不指定此项，则从标准读取数据； [输出文件] 指定输出的文件。如果不指定此选项，则将内容显示到标准输出设备（显示终端）。 语法uniq [-cdu][-f&lt;栏位&gt;][-s&lt;字符位置&gt;][-w&lt;字符位置&gt;][--help][--version][输入文件][输出文件] -c参数得到去除重复行的同时，还在每一行前打印出重复次数 123456[md@vm-kvm5643-app ~]$ uniq tmp2018-01-212019-02-282016-04-302010-10-012019-02-28 123456[md@vm-kvm5643-app ~]$ uniq -c tmp 1 2018-01-21 1 2019-02-28 1 2016-04-30 3 2010-10-01 1 2019-02-28 但是并不相邻的两行，uniq命令是不起作用的，这就需要使用sort 12345[md@vm-kvm5643-app ~]$ sort -n -t&apos;-&apos; -k 1 tmp | uniq2010-10-012016-04-302018-01-212019-02-28 12345[md@vm-kvm5643-app ~]$ sort -n -t&apos;-&apos; -k 1 tmp | uniq -c 3 2010-10-01 1 2016-04-30 1 2018-01-21 2 2019-02-28 找出文件中的重复行 123[md@vm-kvm5643-app ~]$ sort -n -t&apos;-&apos; -k 1 tmp | uniq -d2010-10-012019-02-28]]></content>
      <categories>
        <category>shell</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Linux之df]]></title>
    <url>%2F2019%2F07%2F28%2Flinux%E4%B9%8Bdf%2F</url>
    <content type="text"><![CDATA[disk free基本格式 1df &#123;options&#125; &#123;mount_point_of_filesystem&#125; -a列出所有文件系统的磁盘使用量 12345678910111213141516171819202122[md@vm-kvm5643-app jcf]$ df -aFilesystem 1K-blocks Used Available Use% Mounted on/dev/mapper/vg_root-lv_root 2064208 279252 1680100 15% /proc 0 0 0 - /procsysfs 0 0 0 - /sysdevpts 0 0 0 - /dev/ptstmpfs 4028176 88 4028088 1% /dev/shm/dev/vda1 516040 43396 446432 9% /boot/dev/mapper/vg_root-lv_home 2577136 301320 2146804 13% /home/dev/mapper/vg_root-lv_opt 2064208 320232 1639120 17% /opt/dev/mapper/vg_root-lv_tmp 2064208 68948 1890404 4% /tmp/dev/mapper/vg_root-lv_usr 5160576 2050164 2848268 42% /usr/dev/mapper/vg_root-lv_var 5160576 520464 4377968 11% /var/dev/mapper/vg_app-lv_data 51602044 27507932 21472880 57% /DATAnone 0 0 0 - /proc/sys/fs/binfmt_misc -h以人类易读的方式输出 123456789101112131415161718[md@vm-kvm5643-app jcf]$ df -hFilesystem Size Used Avail Use% Mounted on/dev/mapper/vg_root-lv_root 2.0G 273M 1.7G 15% /tmpfs 3.9G 88K 3.9G 1% /dev/shm/dev/vda1 504M 43M 436M 9% /boot/dev/mapper/vg_root-lv_home 2.5G 295M 2.1G 13% /home/dev/mapper/vg_root-lv_opt 2.0G 313M 1.6G 17% /opt/dev/mapper/vg_root-lv_tmp 2.0G 68M 1.9G 4% /tmp/dev/mapper/vg_root-lv_usr 5.0G 2.0G 2.8G 42% /usr/dev/mapper/vg_root-lv_var 5.0G 509M 4.2G 11% /var/dev/mapper/vg_app-lv_data 50G 27G 21G 57% /DATA]]></content>
      <categories>
        <category>shell</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Linux之sort排序]]></title>
    <url>%2F2019%2F07%2F28%2Flinux%E4%B9%8Bsort%E6%8E%92%E5%BA%8F%2F</url>
    <content type="text"><![CDATA[作用 排序 参数： -b 忽略每行前面开始出的空格字符。 -c 检查文件是否已经按照顺序排序。 -d 排序时，处理英文字母、数字及空格字符外，忽略其他的字符。 -f 排序时，将小写字母视为大写字母。 -i 排序时，除了040至176之间的ASCII字符外，忽略其他的字符。 -m 将几个排序好的文件进行合并。 -M 将前面3个字母依照月份的缩写进行排序。 -n 依照数值的大小排序。 -o&lt;输出文件&gt; 将排序后的结果存入指定的文件。 -r 以相反的顺序来排序。 -t&lt;分隔字符&gt; 指定排序时所用的栏位分隔字符。 +&lt;起始栏位&gt;&lt;结束栏位&gt; 以指定的栏位来排序，范围由起始栏位到结束栏位的前一栏位。 –help 显示帮助。 –version 显示版本信息 1.sort将文件每一行作为一个单位，相互比较，原则上是从首字母向后，按照ASCLL码比较 123456[md@vm-kvm5643-app ~]$ cat tmppythonjavacc++shell 123456[md@vm-kvm5643-app ~]$ sort tmpcc++javapythonshell 2.sort中-u参数是为了取出重复行 12345678[md@vm-kvm5643-app ~]$ cat tmppythonjavacc++shellpythonpython 123456[md@vm-kvm5643-app ~]$ sort -u tmpcc++javapythonshell 3.sort中默认是比较每个字符，所以会出现11在3前面的情况，-n可以指定按照数值比较 123456[md@vm-kvm5643-app ~]$ sort tmp11233278 123456[md@vm-kvm5643-app ~]$ sort -n tmp23113278 4.sort的-o选项是将排序之后的内容输出到指定文件 1234567[md@vm-kvm5643-app ~]$ sort -n tmp -o test[md@vm-kvm5643-app ~]$ cat test23113278 5.排序时-t指定分隔符，-k指定分割之后按照第几个分隔符来排序 123456[md@vm-kvm5643-app ~]$ sort tmp2010-10-012016-04-302018-01-212019-02-28 123456[md@vm-kvm5643-app ~]$ sort -n -t&apos;-&apos; -k 2 tmp2018-01-212019-02-282016-04-302010-10-01]]></content>
      <categories>
        <category>shell</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Python常见问题解析]]></title>
    <url>%2F2019%2F07%2F28%2Fpython%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[解析不定长的iterable： _表示占位符，但只能占一个位置，超过一个位置会抛出ValueError *表示不定长的占位，同时会将不定长的数据放入一个列表中,与切片相比的优势在于可以解析不定长的iterable,当然，_和也能配合`_`使用，表示丢弃掉不定长的iterable123456&gt;&gt;&gt; a = (1, 21, 3, 4, 5, 6)&gt;&gt;&gt; b, *c, _, _ = a&gt;&gt;&gt; print(b)1&gt;&gt;&gt; print(c)[21, 3, 4] 队列与列表的区别： 队列的两端插入和删除的时间复杂度为O(1),列表的两端插入和删除的时间复杂度为O(N) 列表为栈结构，为先进后出，队列为先进先出，列表不限长度，队列默认也为不限长度，但是可以使用maxlen来指定队列长度，超过长度时再append数据时，会先弹出队列第一个数据，再append进队列最后一个数据 队列的相关接口： append appendleft pop popleft extend insert123456789101112131415from collections import deque&gt;&gt;&gt; a = deque(maxlen=3)&gt;&gt;&gt; a.append(1)&gt;&gt;&gt; a.append(2)&gt;&gt;&gt; a.append(3)&gt;&gt;&gt; a.append(4)&gt;&gt;&gt; for i in a:... print(i)...234&gt;&gt;&gt; a[0] = 10&gt;&gt;&gt; print(a)deque([10, 3, 4], maxlen=3) 提取出iterable中的N个最大数或最小数 可用heapq中的nlargest或nsmallest 也可用sorted(iterable, key=key, reverse=True)[:n]或sorted(iterable, key=key, reverse=True)[n:] 处理复杂字典类型12345d = &#123;&#125;for key, value in pairs: if key not in d: d[key] = [] d[key].append(value) 相等 1234from collections import defaultdictd = defaultdict(list)for key, value in pairs: d[key].append(value) 字典排序由于字典是无序的结构，想要控制字典中的顺序，可以使用collections模块中的OrderedDict类，OrderedDict 内部维护着一个根据键插入顺序排序的双向链表，这会在每次插入新元素的时候，会被放入链表尾部。对于一个已经存在的键的重复赋值不会改变键的顺序。ps:值得注意的是，OrderedDict的空间消耗是普通字典类型的两倍 12345678910from collections import OrderedDictimport jsond = OrderedDict()d['foo'] = 1 d['bar'] = 2 d['spam'] = 3 d['grok'] = 4 print(json.dumps(d))# output:&#123;"foo": 1, "bar": 2, "spam": 3, "grok": 4&#125; OrderedDict相关接口： popitem move_to_end copy keys values items zipzip可与字典配合使用用于根据key或者value排序等等 1234567prices = &#123; 'ACME': 45.23, 'AAPL': 612.78, 'IBM': 205.55, 'HPQ': 37.20, 'FB': 10.75 &#125;print(max(zip(prices.values(), prices.keys()))) 值得注意的是：zip()函数创建的是一个只能访问一次的迭代器 123prices_and_names = zip(prices.values(), prices.keys())print(min(prices_and_names)) # OKprint(max(prices_and_names)) # ValueError: max() arg is an empty sequence 字典的key,value和item字典的 keys() 方法返回一个展现键集合的键视图对象。 键视图的一个很少被了解的特性就是它们也支持集合操作，比如集合并、交、差运算。 所以，如果你想对集合的键执行一些普通的集合操作，可以直接使用键视图对象而不用先将它们转换成一个 set。 字典的 items() 方法返回一个包含 (键，值) 对的元素视图对象。 这个对象同样也支持集合操作，并且可以被用来查找两个字典有哪些相同的键值对。 字典的 values() 方法也是类似，但是它并不支持这里介绍的集合操作。 某种程度上是因为值视图不能保证所有的值互不相同 对于重复数据的思考set虽然能很好且很快的取出重复数据，但是带来的问题是，由于set中的元素是无序的，会导致set去重之后的序列也会有被打乱的风险，在此提供的一个思路是，使用生成器函数结合set来去重,且当序列中的元素为unhashable时，同样适用。此方法不仅仅适用于处理普通序列，且可以处理文件中消除重复行等操作。 1234567def dedupe(items, key=None): seen = set() for item in items: val = item if key is None else key(item) if val not in seen: yield item seen.add(val) 对于iterable中的计数问题想要得到iterable中出现次数最多的元素的时候，可以采用sort之后手动计数的方法，更为简单的方式是通过collections模块中的Counter函数。 123456from collections import Counterwords = ( 'look', 'into', 'my', 'eyes', 'look', 'into', 'my', 'eyes','the', 'eyes', 'the', 'eyes', 'the', 'eyes', 'not', 'around', 'the','eyes', "don't", 'look', 'around', 'the', 'eyes', 'look', 'into','my', 'eyes', "you're", 'under' )count = Counter(words)print(count.most_common(3)) # output:[('eyes', 8), ('the', 5), ('look', 4)] Counter提供的常见接口： most_common update 值得注意的是，Counter支持常见的算术运算，包含+,-,and,or等 字典列表排序问题优化在此引入operator模块中的itemgetter函数，f = itemgetter(2)时，调用f(r)返回r[2]，当然，也可以传入多个参数，返回为一个包含多个下标的元祖。对于排序的影响在于，sorted函数排序时，可以指定按照指定的值排序。例如： 1rows_by_fname = sorted(rows, key=itemgetter('fname')) 这表明会按照fname来排序，当然也可以传入多个参数来排序，但是有时候也可以使用lambda 来代替 1rows_by_fname = sorted(rows, key=lambda r: r['fname']) 相比而言，itemgetter方式会更快一些 排序不支持原生比较的对象如果需要排序的是一个实例序列(类似于[User(3), User(5), User(6)]),想通过实例的属性来进行排序时，可使用operator模块中的attrgetter函数,例如: 123456789101112from operator import attrgetterclass User: def __init__(self, user_id): self.user_id = user_id def __repr__(self): return 'User(&#123;&#125;)'.format(self.user_id)def sort_notcompare(): users = [User(23), User(3), User(99)] print(sorted(users, key=attrgetter('user_id'))) 当然也可以通过lambda排序，但lambda排序会慢一点 1sorted(users, key=lambda u: u.user_id) 对于attrgetter函数，提供了如下接口: f = attrgetter(&#39;name&#39;),调用f(r)时，返回f.name f = attrgetter(&#39;name&#39;,&#39;date&#39;)，调用f(r)时，返回`(f.name,f.date)`` f = attrgetter(&#39;name.first&#39;,&#39;name.last&#39;)，调用f(r)时，返回(f.name.first,f.name.last) 字典列表的分组排序当需要对一个字典或实例的列表采用分组排序时，在此引入itertools模块中的groupby函数，作用类似于MYSQL中的group by函数，使用此函数的前提在于，需要进行分组排序的列表，必须提前根据需要分组的元素进行了排序。以下为示例： 123456789101112131415161718192021222324252627282930from operator import itemgetterfrom itertools import groupbyrows = [ &#123;'address': '5412 N CLARK', 'date': '07/01/2012', 'name': 'jack'&#125;,&#123;'address': '5148 N CLARK', 'date': '07/04/2012', 'name': 'mary'&#125;,&#123;'address': '5800 E 58TH', 'date': '07/02/2012', 'name': 'tom'&#125;,&#123;'address': '2122 N CLARK', 'date': '07/03/2012', 'name': 'bob'&#125;,&#123;'address': '5645 N RAVENSWOOD', 'date': '07/02/2012', 'name': 'jay'&#125;,&#123;'address': '1060 W ADDISON', 'date': '07/02/2012', 'name': 'peter'&#125;, &#123;'address': '4801 N BROADWAY', 'date': '07/01/2012', 'name': 'jack'&#125;, &#123;'address': '1039 W GRANVILLE', 'date': '07/04/2012', 'name': 'jan'&#125;,]# Sort by the desired field firstrows.sort(key=itemgetter('date'))for date, items in groupby(rows, key=itemgetter('date')): print(date) for i in items: print(' ', i)#output'''07/01/2012 &#123;'address': '5412 N CLARK', 'date': '07/01/2012', 'name': 'jack'&#125; &#123;'address': '4801 N BROADWAY', 'date': '07/01/2012', 'name': 'jack'&#125;07/02/2012 &#123;'address': '5800 E 58TH', 'date': '07/02/2012', 'name': 'tom'&#125; &#123;'address': '5645 N RAVENSWOOD', 'date': '07/02/2012', 'name': 'jay'&#125; &#123;'address': '1060 W ADDISON', 'date': '07/02/2012', 'name': 'peter'&#125;07/03/2012 &#123;'address': '2122 N CLARK', 'date': '07/03/2012', 'name': 'bob'&#125;07/04/2012 &#123;'address': '5148 N CLARK', 'date': '07/04/2012', 'name': 'mary'&#125; &#123;'address': '1039 W GRANVILLE', 'date': '07/04/2012', 'name': 'jan'&#125;''' 过滤列表中的元素提供以下几种思路 使用列表生成式，优点在于代码量小，缺点在于过滤复杂条件麻烦且不宜读，而且会占用大量内存 使用filter函数，返回一个迭代器，而且可以将复杂的过滤条件封装在一个函数中 使用itertools模块中的compress函数，也返回一个迭代器，而且采用的是一个Boolean列表来过滤另外一个列表 列表生成式过滤数据: 12&gt;&gt;&gt; mylist = [1, 4, -5, 10, -7, 2, 3, -1]&gt;&gt;&gt; [n for n in mylist if n &gt; 0] filter函数： 123456789values = ['1', '2', '-3', '-', '4', 'N/A', '5']def is_int(val): try: x = int(val) return True except ValueError: return Falseivals = list(filter(is_int, values))print(ivals) compress函数： 1234567891011addresses = [ '5412 N CLARK', '5148 N CLARK', '5800 E 58TH', '2122 N CLARK', '5645 N RAVENSWOOD', '1060 W ADDISON', '4801 N BROADWAY', '1039 W GRANVILLE',]counts = [ 0, 3, 10, 4, 1, 7, 6, 1] 123456&gt;&gt;&gt; from itertools import compress&gt;&gt;&gt; more5 = [n &gt; 5 for n in counts]&gt;&gt;&gt; more5[False, False, True, False, False, True, True, False]&gt;&gt;&gt; list(compress(addresses, more5))['5800 E 58TH', '1060 W ADDISON', '4801 N BROADWAY'] 过滤字典可采用字典推导式 合并多个字典 可采用dict中的update来讲两个字典合并为一个字典 可使用collections模块中的ChainMap函数，而且该函数只会创建一个临时的合并字典，以供数据采用，所以效率会更高一点 12a = &#123;'x': 1, 'z': 3 &#125;b = &#123;'y': 2, 'z': 4 &#125; 1234from collections import ChainMapc = ChainMap(a,b)print(c['x']) # Outputs 1 (from a)print(c['y']) # Outputs 2 (from b) ChainMap提供以下接口: new_child新加入一个字典进入一个ChainMap parents返回父节点 maps返回字典列表形式 如果出现重复键，那么第一次出现的映射值会被返回 字符串分割问题 大多数情况可以使用str.splite满足要求 更好的选择可以是使用re模块的splite函数 1234&gt;&gt;&gt; line = 'asdf fjdk; afed, fjek,asdf, foo'&gt;&gt;&gt; import re&gt;&gt;&gt; re.split(r'[;,\s]\s*', line)['asdf', 'fjdk', 'afed', 'fjek', 'asdf', 'foo'] 值得注意的是，需要选择是否使用括号来捕捉分组。如果又想使用括号，并且不想捕捉括号内分组，可使用?:...模式 12&gt;&gt;&gt; re.split(r'(?:,|;|\s)\s*', line)['asdf', 'fjdk', 'afed', 'fjek', 'asdf', 'foo'] re.splite提供了maxsplite参数，默认为0，表示将字符串中所有值都切分，如果不为0，表示将返回包含给定个数字符串的列表，剩余元素将作为最后一个元素输出 12&gt;&gt;&gt; print(re.split('(?:\s|,|;)\s*', line, 3))['asdf', 'fjdk', 'afed', 'fjek,asdf, foo'] 关于字符串开头或结尾的思考str提供了startwsitch和endswitch函数来检查字符串的开头或结尾，值得注意的是，传参只能为str类型或tuple类型。当然，参数也支持start和end，作为开始和结束位置的检查点 对于开头或结尾的处理，使用切片或者是正则表达式也是可以的，但是使用startwsitch和endswitch会更快且更方便 123&gt;&gt;&gt; url = 'http://www.python.org'&gt;&gt;&gt; print(url.startswith('www', 7)) True 关于字符串替换 对于一般情况，可使用str.replace()即可满足 更多情况，需要使用re模块的sub函数，来个性化定制需要替换的值 str提供了translate()来替换和清理较为复杂的字符串 1234&gt;&gt;&gt; text = 'Today is 11/27/2012. PyCon starts 3/13/2013.'&gt;&gt;&gt; import re&gt;&gt;&gt; re.sub(r'(\d+)/(\d+)/(\d+)', r'\3-\1-\2', text)'Today is 2012-11-27. PyCon starts 2013-3-13.' 反斜杠数字比如 \3 指向前面模式的捕获组号。如果需要使用相同模式来做多次替换，可以考虑先编译来提升性能 而对于更加复杂的替换，可以使用一个回调函数来替代 1234567&gt;&gt;&gt; from calendar import month_abbr&gt;&gt;&gt; def change_date(m):... mon_name = month_abbr[int(m.group(1))]... return '&#123;&#125; &#123;&#125; &#123;&#125;'.format(m.group(2), mon_name, m.group(3))...&gt;&gt;&gt; datepat.sub(change_date, text)'Today is 27 Nov 2012. PyCon starts 13 Mar 2013.' 如果想要查看发生了多少次替换，可用re.subn()来代替 12345&gt;&gt;&gt; newtext, n = datepat.subn(r'\3-\1-\2', text)&gt;&gt;&gt; newtext'Today is 2012-11-27. PyCon starts 2013-3-13.'&gt;&gt;&gt; n2 ps:对于大多数正则表达式来说，都提供一个flags参数，flags=re.IGNORECASE是，表示忽略大小写 多行匹配模式对于正则表达式来说，点(.)并不支持换行符匹配，对于需要多行匹配的问题，提供两种思路： 在正则表达式中加入换行匹配规则，例：re.compile(r&#39;/\*((?:.|\n)*?)\*/&#39;) re.compile()函数接受一个参数叫re.DOTALL,表示点(.)接受包括换行符在内的任意字符 123&gt;&gt;&gt; comment = re.compile(r'/\*(.*?)\*/', re.DOTALL)&gt;&gt;&gt; comment.findall(text2)[' this is a\n multiline comment '] 字符串对齐 str提供了ljust() , rjust() 和 center() 方法可以指定对齐方式 使用foemat()函数对齐 123456&gt;&gt;&gt; format(text, '&gt;20')' Hello World'&gt;&gt;&gt; format(text, '&lt;20')'Hello World '&gt;&gt;&gt; format(text, '^20')' Hello World ' 填充字符 12&gt;&gt;&gt; format(text, '=&gt;20s')'=========Hello World' 格式化多个值 12&gt;&gt;&gt; '&#123;:&gt;10s&#125; &#123;:&gt;10s&#125;'.format('Hello', 'World')' Hello World' 关于字符串拼接的思考 最主流的还是使用str提供的join函数 对于简单的拼接也可使用+和format()函数 对于性能的考虑 使用+连接符去操作大量字符串的效率非常低下，因为加号会带来内存复制和垃圾回收，应尽量避免写下面的函数 123s = ''for p in parts: s += p 更为聪明的做法是使用生成器表达式 123&gt;&gt;&gt; data = ['ACME', 50, 91.1]&gt;&gt;&gt; ','.join(str(d) for d in data)'ACME,50,91.1' 同时，注意不必要的字符连接 123print(a + ':' + b + ':' + c) # Uglyprint(':'.join([a, b, c])) # Still uglyprint(a, b, c, sep=':') # Better 关于字节类型的字符串(byte类型) 字节类型的字符串支持大部分字符串的操作，例如：replace,splite,切片,正则表达式等 值得注意的是，字节类型的字符串通过索引返回的是整数而非操作数，例如 123&gt;&gt;&gt; a = b'hello'&gt;&gt;&gt; print(a[0])104 ps:与python2对比，字节类型的字符串与普通字符串并无区别，例如： 12345&gt;&gt;&gt; print type(bytes('hello'))&lt;type 'str'&gt;&gt;&gt;&gt; a = b'hello'&gt;&gt;&gt; print a[0]h 字节类型与字符串类型的相互转换： 123456&gt;&gt;&gt; a = b'hello'&gt;&gt;&gt; b = a.decode('ascii')&gt;&gt;&gt; print(b)hello&gt;&gt;&gt; print(b.encode('ascii'))b'hello' 关于数字的处理四舍五入 一般情况，可用python内置函数round(value, ndigits)即可,ndigits表示对于小数点后几位来四舍五入，当然，也可为负数，表示对整数后几位进行四舍五入 浮点数精度问题 123456&gt;&gt;&gt; a = 4.2&gt;&gt;&gt; b = 2.1&gt;&gt;&gt; print(a+b)6.300000000000001&gt;&gt;&gt; print((a+b) == 6.3)False 这些错误是由底层CPU和IEEE 754标准通过自己的浮点单位去执行算术时的特征.所以没法自己去避免这些误差 想要无误差处理浮点数精度问题，可参考decimal模块Decimal函数 1234from decimal import Decimala = Decimal('4.2')b = Decimal('2')print(a+b) #output:6.2 随机选择 random.choice()表示随机选择一个字符 1234&gt;&gt;&gt; import random&gt;&gt;&gt; values = [1, 2, 3, 4, 5, 6]&gt;&gt;&gt; random.choice(values)2 random.sample()表示随机选择n个字符 12&gt;&gt;&gt; random.sample(values, 2)[6, 2] random.shuffle()表示打乱排序 123&gt;&gt;&gt; random.shuffle(values)&gt;&gt;&gt; values[2, 4, 6, 5, 3, 1] random.randint() 表示随机选择一个整数 12&gt;&gt;&gt; random.randint(0,10)2 random.random() 表示随机生成一个从0到1的浮点数 12&gt;&gt;&gt; random.random()0.9406677561675867 random.getrandbits(k) 表示随机生成k为二进制随机数的整数 12&gt;&gt;&gt; random.getrandbits(10)512 PS:值得注意的是，random模块采用的是Mersenne Twister 算法来计算生成随机数。这是一个确定性算法， 但是你可以通过 random.seed() 函数修改初始化种子。所以，对于安全性要求高的应尽量避免random模块的使用 时间与日期时间段 123456&gt;&gt;&gt; from datetime import timedelta&gt;&gt;&gt; a = timedelta(days=2, hours=6)&gt;&gt;&gt; print(a)2 days, 6:00:00&gt;&gt;&gt; print(a.days)2 值得注意的是，timedelta函数并没有提供年和月的时间段函数，使用timedelta的优势在于可是与datetime的时间做运算 1234&gt;&gt;&gt; from datetime import datetime&gt;&gt;&gt; a = datetime(2012, 9, 23)&gt;&gt;&gt; print(a + timedelta(days=10))2012-10-03 00:00:00 迭代器与生成器使用生成器实现深度优先算法 123456789101112131415161718192021222324252627282930313233class Node: def __init__(self, value): self._value = value self._children = [] def __repr__(self): return 'Node(&#123;!r&#125;)'.format(self._value) def add_child(self, node): self._children.append(node) def __iter__(self): return iter(self._children) def depth_first(self): yield self for c in self: yield from c.depth_first()# Exampleif __name__ == '__main__': root = Node(0) child1 = Node(1) child2 = Node(2) root.add_child(child1) root.add_child(child2) child1.add_child(Node(3)) child1.add_child(Node(4)) child2.add_child(Node(5)) for ch in root.depth_first(): print(ch) # Outputs Node(0), Node(1), Node(3), Node(4), Node(2), Node(5) 反向迭代 对于一个iterable可以使用reversed()函数来实现反向迭代，例如 1print(reversed([1, 2, 3])) 当然，列表的内存消耗过大，如果处理大文件形成的列表可以转换为元祖进行处理。 除此之外，可以使用自定义类中的魔法函数__reversed__()来实现反向迭代，例如 1234567891011121314151617class Countdown: def __init__(self, start): self.start = start # Forward iterator def __iter__(self): n = self.start while n &gt; 0: yield n n -= 1 # Reverse iterator def __reversed__(self): n = 1 while n &lt;= self.start: yield n n += 1 相比而言，反向迭代器运行非常高效，因为它不再需要将数据填充到一个列表中然后再去反向迭代这个列表。 生成器与迭代器切片 生成器与迭代器并不能像普通列表切片一样，可以通过itertools模块中的islice()函数实现切片，例 123456from itertools import islicea = iter([1, 2, 3, 4, 5])b = islice(a, 1, 2)for i in b: print(i)#output:2 值得注意的是，islice()是会消耗掉迭代器或生成器中的值，因为必须考虑到迭代器是一个不可逆的过程 123for i in a: print(i)#output:3, 4, 5 处理文件中需要跳过的部分 顾名思义，跳过不需要的部分，可使用itertools模块中的dropwhile()函数，例如，可跳过注释部分，代码如下 123with open(path, 'r') as f: for line in dropwhile(lambda line: line.startswith('#'), f): print(line, end='') 值得注意的是，dropwhile函数只能跳过文章开头的部分，并不能跳过中间满足条件的部分 如果想要提取文件中的部分值，仍可通过islice函数获取 123with open(path, 'r') as f: for line in islice(f, 2, 5): print(line, end='') 如果想要跳过所有满足条件的注释行，可改动下代码 1234with open(path, 'r') as f: lines = (line for line in f if not line.startswith('#')) for line in lines: print(line, end='') 同时迭代多个序列 简言之，就是将多可序列+起来，可使用内置函数zip()，zip()函数返回一个迭代器，并且迭代长度和参数中最短序列长度一致 12345a = [1, 2, 3]b = ['m', 'n']for i in zip(a, b): print(i)#output:(1, 'm'),(2, 'n') 当然，返回参数中最大长度也是可以的，可以使用itertools模块中的zip_longest函数 123456from itertools import zip_longesta = [1, 2, 3]b = ['m', 'n']for i in zip_longest(a, b, fillvalue=0): print(i)#output:(1, 'm'),(2, 'n'),(3, 0) 关于多个可迭代对象求和 1234a = [1, 2, 4]b = [5, 6, 7]for i in a+b: print(i) 通过这种常规的方式是可以达到目的的，有两个问题： 列表过大时，消耗内存也会太大 a,b不是同一种类型时，这种操作会抛异常 所以，在此引入itertools模块中的chain函数，上面的可改为： 12345from itertools import chaina = [1, 2, 4]b = [5, 6, 7]for i in chain(a, b): print(i) chain的优势在于，可处理两种不同类型的iterable，且会省内存 关于嵌套序列的处理 关于嵌套序列的处理存在多种方法，在此使用生成器的方式，在于节省内存且代码优雅，例如 12345678910111213from collections import Iterabledef flatten(items, ignore_types=(str, bytes)): for x in items: if isinstance(x, Iterable) and not isinstance(x, ignore_types): yield from flatten(x) else: yield xitems = [1, 2, [3, 4, [5, 6], 7], 8]# Produces 1 2 3 4 5 6 7 8for x in flatten(items): print(x) 文件与IO关于文件读取 open()函数打开文件时，对于换行符的识别在UNIX和Windows下的识别是不一样的(分别为\n和\r\n),默认情况下，python会统一处理换行符，并在输出时，将换行符替换为\n，当然也可以手动指定换行符，使用newline参数指定 在文件编码时，可能出现编码和解码方式不一样而导致打开文件失败的情况，可指定error参数，来处理打开失败的情况 123with open(path, 'rt', encoding='ascii', errors='replace') as f: print([f.read()])#output:['172.24.107.153\n������'] 123with open(path, 'rt', encoding='ascii', errors='ignore') as f: print([f.read()])#output:['172.24.107.153\n'] 关于print函数 1print(self, *args, sep=' ', end='\n', file=None) 其中seq是指定多个参数时的分隔符，默认为空格；end是在输出的末尾加上需要的字符，使用end参数在输出中禁止换行，默认为换行符；file是指流文件，可以用作重定向字符到文件中 1print(2019, 7, 24, sep='-', end='!!!') 值得注意的是，对于参数合并时，&#39;&#39;.join也支持字符串合并，但是仅支持字符串合并，而sep参数能将不同类型的参数合并在一起 处理文件不存在时才能写入 言下之意是，在目录中不能存在这个文件名，如果存在就会抛异常，通常会存在以下两种方案： 通过os.path.exists判断该文件是否存在 open函数提供了x参数，表示会创建一个文件，并以写的方式打开，且文件如果已经存在会抛异常 12with open(path, 'x') as f: f.write('write text') 值得注意的是，x参数是python3才引入的，之前版本并不支持 字符串I/O操作 在涉及到需要创建一个文件来存储数据时，通常存在以下两种方式： 在本地磁盘创建一个文件，以写的方式放入数据，用完之后再删了，这样的有点在于不会存在太大的内存限制，缺点在于可能会存在读写速度问题 使用io.StringIO()函数来处理，优势在于将数据存在内存中，读写速度会高于磁盘读写，缺点在于，数据量过大可能会带来内存问题 1234567891011121314# write a file&gt;&gt;&gt; from io import StringIO&gt;&gt;&gt;&gt;&gt;&gt; s = StringIO()&gt;&gt;&gt; s.write('这是一个文件io操作\n')11&gt;&gt;&gt; print('今天是星期四\n',file=s)&gt;&gt;&gt; print('明天星期五\n',end='',file=s)&gt;&gt;&gt; s.getvalue()'这是一个文件io操作\n今天是星期四\n\n明天星期五\n'# read a file&gt;&gt;&gt; s = StringIO('today\n')&gt;&gt;&gt; s.read(3)'tod' 同理，涉及到二进制数据时，需要用BytesIO函数来代替 需要注意的是， StringIO 和 BytesIO 实例并没有正确的整数类型的文件描述符。 因此，它们不能在那些需要使用真实的系统级文件如文件，管道或者是套接字的程序中使用。 固定字符串长度迭代文件 12345678from functools import partialRECORD_SIZE = 32with open('somefile.data', 'rb') as f: records = iter(partial(f.read, RECORD_SIZE), b'') for r in records: ... 这样会不断产生一个固定大小的数据块，当然也可以自己根据需要产生的数据块进行逻辑处理。 需要注意的地方在于，普通文本的处理方式，默认迭代方法为一行一行的读取，这通常是更普遍的做法 关于os.path处理文件路径 123456789101112131415161718&gt;&gt;&gt; import os&gt;&gt;&gt; path = '/Users/beazley/Data/data.csv'&gt;&gt;&gt; os.path.basename(path) # 获取文件名'data.csv'&gt;&gt;&gt; os.path.dirname(path) # 获取文件目录'/Users/beazley/Data'&gt;&gt;&gt; os.path.join('tmp', 'data', os.path.basename(path)) # 组合路径'tmp/data/data.csv'&gt;&gt;&gt; path = '~/Data/data.csv'&gt;&gt;&gt; os.path.expanduser(path) # 获取文件绝对路径'/Users/beazley/Data/data.csv'&gt;&gt;&gt; os.path.splitext(path) # 分离扩展名和路径('~/Data/data', '.csv') 123456789101112&gt;&gt;&gt; os.path.isfile('/etc/passwd')True&gt;&gt;&gt; os.path.isdir('/etc/passwd')False&gt;&gt;&gt; os.path.islink('/usr/local/bin/redis-sentinel')True&gt;&gt;&gt; os.path.realpath('/usr/local/bin/redis-sentinel')'/usr/local/bin/redis-server'&gt;&gt;&gt; os.path.getsize('/usr/local/bin/redis-sentinel')2109680&gt;&gt;&gt; os.path.getmtime('/usr/local/bin/redis-sentinel')1472713782.0 不同类型文件格式处理csv文件处理 读取csv文件 包含两种读取方式，可通过字符串读取和字典读取 12345import csvwith open('stocks.csv') as f: f_csv = csv.reader(f) headers = next(f_csv) for row in f_csv: 1234import csvwith open('stocks.csv') as f: f_csv = csv.DictReader(f) for row in f_csv: 写入csv文件 也包含两种写入方式，可通过普通字符串写入和字典写入 1234with open('stocks.csv','w') as f: f_csv = csv.writer(f) f_csv.writerow(headers) f_csv.writerows(rows) 1234with open('stocks.csv','w') as f: f_csv = csv.DictWriter(f, headers) f_csv.writeheader() f_csv.writerows(rows) csv文件中分割字符串 csv.reader()函数中带有delimiter参数可指定分割方式 123with open('stock.tsv') as f: f_tsv = csv.reader(f, delimiter='\t') for row in f_tsv: csv文件字符转换 由于csv中读取的数据都为字符串类型，只能手动操作数据类型转换 123456col_types = [str, float, str, str, float, int]with open('stocks.csv') as f: f_csv = csv.reader(f) headers = next(f_csv) for row in f_csv: row = tuple(convert(value) for convert, value in zip(col_types, row)) 当然这样存在风险性，是因为实际情况中csv文件或多或少存在缺失的数据，这样可能导致数据转换抛出异常 处理json数据 处理xml文件 python处理xml文件通常存在多种方式，这里分别以处理简单文件和大文件为例 处理一般文件，通常使用ElementTree模块,python3.3之后会自动寻找可用的C库来加快速度 1234try: import xml.etree.cElementTree as ETexcept ImportError: import xml.etree.ElementTree as ET 解析根节点 123tree = ET.parse('111.xml')root = tree.getroot()#&lt;Element data at 0x1f74dabef48&gt;, so root is data 而获取root的原因在于方便后面解析使用, 通常情况，xml结构标识为&lt;tag attrib1=1&gt;text&lt;/tag&gt;tail 1234567#source data:&lt;country name="Liechtenstein"&gt;&gt;&gt;&gt; root[0].attrib #获取属性&#123;'name': 'Liechtenstein'&#125;&gt;&gt;&gt; root[0][0].text #获取文本2&gt;&gt;&gt; root[0][0].tag #获取标签rank 当然，root也是可以迭代的 12for i in root: print(i.tag, i.attrib, i.text, i.tail, sep=';', end='') 也可以是根据某一个标签进行迭代 12345678for i in root.findall('country'): #遍历所有符合条件子节点...for i in root.find('country'): #遍历第一个符合条件子节点...for i in root.findtext('country'): #只遍历文本...for i in root.iter('country'): #以当前节点为树节点... 修改xml 需要注意的是，xml中所有字符均为字符串类型，需要注意字符转换 12345rank.text = str(new_rank) # 必须将int转为strrank.set("updated", "yes") # 添加属性del rank.attrib['updated'] #删除属性tree.write('111.xml') #将数据写入磁盘root.remove(country) #删除子节点 修改之后的内容只是放在内存中，所以需要将内存里面的数据保存到磁盘中 处理大型xml文件 当然，处理大型文档，除了使用固有的函数模块之外，还可以使用普通文档解析方式，这样只不过会导致取值更麻烦而已 其实只要一想到处理大型数据，就应该第一时间想到迭代器或者生成器 1234567891011121314151617181920212223from xml.etree.ElementTree import iterparsedef parse_and_remove(filename, path): path_parts = path.split('/') doc = iterparse(filename, ('start', 'end')) # Skip the root element next(doc) tag_stack = [] elem_stack = [] for event, elem in doc: if event == 'start': tag_stack.append(elem.tag) elem_stack.append(elem) elif event == 'end': if tag_stack == path_parts: yield elem elem_stack[-2].remove(elem) try: tag_stack.pop() elem_stack.pop() except IndexError: pass iterparse() 方法允许对XML文档进行增量操作。 使用时，你需要提供文件名和一个包含下面一种或多种类型的事件列表： start, end, start-ns 和 end-ns 。由 iterparse() 创建的迭代器会产生形如 (event, elem) 的元组， 其中 event 是上述事件列表中的某一个，而 elem 是相应的XML元素。 start 事件在某个元素第一次被创建并且还没有被插入其他数据(如子元素)时被创建。 而 end 事件在某个元素已经完成时被创建。 在 yield 之后的下面这个语句才是使得程序占用极少内存的ElementTree的核心特性： 1elem_stack[-2].remove(elem) 这个语句使得之前由 yield 产生的元素从它的父节点中删除掉。 假设已经没有其它的地方引用这个元素了，那么这个元素就被销毁并回收内存。 对节点的迭代式解析和删除的最终效果就是一个在文档上高效的增量式清扫过程。 文档树结构从始自终没被完整的创建过。尽管如此，还是能通过上述简单的方式来处理这个XML数据。 将字典类型数据转换为xml 存在两种解决方案： 手动构造，以字符串的format函数替代的方式来构造，不过这样显得有点蠢 使用xml.etree.ElementTree模块中的Element函数 123456789101112from xml.etree.ElementTree import Element, tostringdefdict_to_xml(tag, d): elem = Element(tag) for key, val in d.items(): child = Element(key) child.text = str(val) elem.append(child) return elems = &#123;'name': 'GOOG', 'shares': 100, 'price': 490.1&#125;e = dict_to_xml('stock', s) #&lt;Element 'stock' at 0x000001CE0548C908&gt;print(tostring(e).decode('utf-8'))#&lt;stock&gt;&lt;name&gt;GOOG&lt;/name&gt;&lt;shares&gt;100&lt;/shares&gt;&lt;price&gt;490.1&lt;/price&gt;&lt;/stock&gt; 这样做的目的在于，可以通过查询数据库中的值放进字典中，利用字典生成xml文件]]></content>
      <categories>
        <category>python</category>
      </categories>
  </entry>
</search>
