<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[MYSQL基础]]></title>
    <url>%2F2019%2F07%2F28%2FSQL%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[查询1.排序排序之后如果想要进一步排序，可以继续添加列名。例如，ORDER BY score,name表示先按照score排序，遇到相同的score时，再按照name排序。如果有WHERE语句时，必须放在WHERE语句之后 升序：ORDER BY和ORDER BY ASC相同 降序：ORDER BY DESC 2.分页可以通过LIMIT &lt;M&gt; OFFSET &lt;N&gt;查询分页数据，表示从N开始，提取M条数据，可简写为LIMIT M,N，且N越大，查询效率越低。 3.聚合查询 函数 说明 COUNT() 获取行数 SUM() 计算某一列的合计值，该列必须为数值类型 AVG() 计算某一列的平均值，该列必须为数值类型 MAX() 计算某一列的最大值 MIN() 计算某一列的最小值 注意： MAX()和MIN()函数并不限于数值类型。如果是字符类型，MAX()和MIN()会返回排序最后和排序最前的字符。 如果聚合查询的WHERE条件没有匹配到任何行，COUNT()会返回0，而SUM()、AVG()、MAX()和MIN()会返回NULL 分组 例：使用一条SELECT查询查出每个班级男生和女生的平均分 1`SELECT class_id,gender,AVG(score) from students group by class_id,gender;` 4.连接查询左连接语法规则： 1SELECT ... FROM [表1] INNER JOIN [表2] ON [条件...] 右链接 RIGHT OUTER JOIN返回右表都存在的行。如果某一行仅在右表存在，那么结果集就会以NULL填充剩下的字段。 LEFT OUTER JOIN则返回左表都存在的行。 使用FULL OUTER JOIN，它会把两张表的所有记录全部选择出来，并且，自动把对方不存在的列填充为NULL 修改数据1.增加数据使用INSERT，我们就可以一次向一个表中插入一条或多条记录。 1INSERT INTO [表名] (字段1, 字段2, ...) VALUES (值1, 值2, ...); 2.修改数据使用UPDATE，我们就可以一次更新表中的一条或多条记录。 1UPDATE [表名] SET 字段1=值1, 字段2=值2, ... WHERE ...; 3.删除数据使用DELETE，我们就可以一次删除表中的一条或多条记录。 1DELETE FROM [表名] WHERE ...; 替换 1REPLACE INTO students (id, class_id, name, gender, score) VALUES (1, 1, &apos;小明&apos;, &apos;F&apos;, 99); 更新场景:希望插入一条新记录（INSERT），但如果记录已经存在，就更新该记录，此时，可以使用INSERT INTO ... ON DUPLICATE KEY UPDATE ...语句 1INSERT INTO students (id, class_id, name, gender, score) VALUES (1, 1, &apos;小明&apos;, &apos;F&apos;, 99) ON DUPLICATE KEY UPDATE name=&apos;小明&apos;, gender=&apos;F&apos;, score=99; 插入和忽略场景:希望插入一条新记录（INSERT），但如果记录已经存在，就啥事也不干直接忽略，此时，可以使用INSERT IGNORE INTO ...语句 1INSERT IGNORE INTO students (id, class_id, name, gender, score) VALUES (1, 1, &apos;小明&apos;, &apos;F&apos;, 99); 快照场景:想要对一个表进行快照，即复制一份当前表的数据到一个新表，可以结合CREATE TABLE和SELECT 12-- 对class_id=1的记录进行快照，并存储为新students_of_class1:CREATE TABLE students_of_class1 SELECT * FROM students WHERE class_id=1; 事务把多条数据库语句作为一个整体进行操作的功能，成为数据库事务，数据库事务可以确保事务范围内的所有操作全部成功或者全部失败，如果全部失败，那么效果就和没执行这些sql语句一样。 数据库事务具有ACID这4个特性： A：Atomic，原子性，将所有SQL作为原子工作单元执行，要么全部执行，要么全部不执行； C：Consistent，一致性，事务完成后，所有数据的状态都是一致的，即A账户只要减去了100，B账户则必定加上了100； I：Isolation，隔离性，如果有多个事务并发执行，每个事务作出的修改必须与其他事务隔离； D：Duration，持久性，即事务完成后，对数据库数据的修改被持久化存储。 对于单条SQL语句，数据库系统自动将其作为一个事务执行，这种事务被称为隐式事务要手动把多条SQL语句作为一个事务执行，使用BEGIN开启一个事务，使用COMMIT提交一个事务，这种事务被称为显式事务 1234BEGIN;UPDATE accounts SET balance = balance - 100 WHERE id = 1;UPDATE accounts SET balance = balance + 100 WHERE id = 2;COMMIT; 有些时候，我们希望主动让事务失败，这时，可以用ROLLBACK回滚事务，整个事务会失败 1234BEGIN;UPDATE accounts SET balance = balance - 100 WHERE id = 1;UPDATE accounts SET balance = balance + 100 WHERE id = 2;ROLLBACK;]]></content>
      <categories>
        <category>MYSQL</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Linux之df]]></title>
    <url>%2F2019%2F07%2F28%2Flinux%E4%B9%8Bdf%2F</url>
    <content type="text"><![CDATA[disk free基本格式 1df &#123;options&#125; &#123;mount_point_of_filesystem&#125; -a列出所有文件系统的磁盘使用量 12345678910111213141516171819202122[md@vm-kvm5643-app jcf]$ df -aFilesystem 1K-blocks Used Available Use% Mounted on/dev/mapper/vg_root-lv_root 2064208 279252 1680100 15% /proc 0 0 0 - /procsysfs 0 0 0 - /sysdevpts 0 0 0 - /dev/ptstmpfs 4028176 88 4028088 1% /dev/shm/dev/vda1 516040 43396 446432 9% /boot/dev/mapper/vg_root-lv_home 2577136 301320 2146804 13% /home/dev/mapper/vg_root-lv_opt 2064208 320232 1639120 17% /opt/dev/mapper/vg_root-lv_tmp 2064208 68948 1890404 4% /tmp/dev/mapper/vg_root-lv_usr 5160576 2050164 2848268 42% /usr/dev/mapper/vg_root-lv_var 5160576 520464 4377968 11% /var/dev/mapper/vg_app-lv_data 51602044 27507932 21472880 57% /DATAnone 0 0 0 - /proc/sys/fs/binfmt_misc -h以人类易读的方式输出 123456789101112131415161718[md@vm-kvm5643-app jcf]$ df -hFilesystem Size Used Avail Use% Mounted on/dev/mapper/vg_root-lv_root 2.0G 273M 1.7G 15% /tmpfs 3.9G 88K 3.9G 1% /dev/shm/dev/vda1 504M 43M 436M 9% /boot/dev/mapper/vg_root-lv_home 2.5G 295M 2.1G 13% /home/dev/mapper/vg_root-lv_opt 2.0G 313M 1.6G 17% /opt/dev/mapper/vg_root-lv_tmp 2.0G 68M 1.9G 4% /tmp/dev/mapper/vg_root-lv_usr 5.0G 2.0G 2.8G 42% /usr/dev/mapper/vg_root-lv_var 5.0G 509M 4.2G 11% /var/dev/mapper/vg_app-lv_data 50G 27G 21G 57% /DATA]]></content>
      <categories>
        <category>shell</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Linux之sort排序]]></title>
    <url>%2F2019%2F07%2F28%2Flinux%E4%B9%8Bsort%E6%8E%92%E5%BA%8F%2F</url>
    <content type="text"><![CDATA[作用 排序 参数： -b 忽略每行前面开始出的空格字符。 -c 检查文件是否已经按照顺序排序。 -d 排序时，处理英文字母、数字及空格字符外，忽略其他的字符。 -f 排序时，将小写字母视为大写字母。 -i 排序时，除了040至176之间的ASCII字符外，忽略其他的字符。 -m 将几个排序好的文件进行合并。 -M 将前面3个字母依照月份的缩写进行排序。 -n 依照数值的大小排序。 -o&lt;输出文件&gt; 将排序后的结果存入指定的文件。 -r 以相反的顺序来排序。 -t&lt;分隔字符&gt; 指定排序时所用的栏位分隔字符。 +&lt;起始栏位&gt;&lt;结束栏位&gt; 以指定的栏位来排序，范围由起始栏位到结束栏位的前一栏位。 –help 显示帮助。 –version 显示版本信息 1.sort将文件每一行作为一个单位，相互比较，原则上是从首字母向后，按照ASCLL码比较 123456[md@vm-kvm5643-app ~]$ cat tmppythonjavacc++shell 123456[md@vm-kvm5643-app ~]$ sort tmpcc++javapythonshell 2.sort中-u参数是为了取出重复行 12345678[md@vm-kvm5643-app ~]$ cat tmppythonjavacc++shellpythonpython 123456[md@vm-kvm5643-app ~]$ sort -u tmpcc++javapythonshell 3.sort中默认是比较每个字符，所以会出现11在3前面的情况，-n可以指定按照数值比较 123456[md@vm-kvm5643-app ~]$ sort tmp11233278 123456[md@vm-kvm5643-app ~]$ sort -n tmp23113278 4.sort的-o选项是将排序之后的内容输出到指定文件 1234567[md@vm-kvm5643-app ~]$ sort -n tmp -o test[md@vm-kvm5643-app ~]$ cat test23113278 5.排序时-t指定分隔符，-k指定分割之后按照第几个分隔符来排序 123456[md@vm-kvm5643-app ~]$ sort tmp2010-10-012016-04-302018-01-212019-02-28 123456[md@vm-kvm5643-app ~]$ sort -n -t&apos;-&apos; -k 2 tmp2018-01-212019-02-282016-04-302010-10-01]]></content>
      <categories>
        <category>shell</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Linux之uniq]]></title>
    <url>%2F2019%2F07%2F28%2Flinux%E4%B9%8Buniq%2F</url>
    <content type="text"><![CDATA[作用 Linux uniq 命令用于检查及删除文本文件中重复出现的行列，一般与 sort 命令结合使用。 uniq 可检查文本文件中重复出现的行列。参数 -c 在每列旁边显示该行重复出现的次数。 -d 仅显示重复出现的行列。 -f&lt;栏位&gt; 忽略比较指定的栏位。 -s&lt;字符位置&gt; 忽略比较指定的字符。 -u 仅显示出一次的行列。 -w[字符位置] 指定要比较的字符。 –help 显示帮助。 –version 显示版本信息。 [输入文件] 指定已排序好的文本文件。如果不指定此项，则从标准读取数据； [输出文件] 指定输出的文件。如果不指定此选项，则将内容显示到标准输出设备（显示终端）。 语法uniq [-cdu][-f&lt;栏位&gt;][-s&lt;字符位置&gt;][-w&lt;字符位置&gt;][--help][--version][输入文件][输出文件] -c参数得到去除重复行的同时，还在每一行前打印出重复次数 123456[md@vm-kvm5643-app ~]$ uniq tmp2018-01-212019-02-282016-04-302010-10-012019-02-28 123456[md@vm-kvm5643-app ~]$ uniq -c tmp 1 2018-01-21 1 2019-02-28 1 2016-04-30 3 2010-10-01 1 2019-02-28 但是并不相邻的两行，uniq命令是不起作用的，这就需要使用sort 12345[md@vm-kvm5643-app ~]$ sort -n -t&apos;-&apos; -k 1 tmp | uniq2010-10-012016-04-302018-01-212019-02-28 12345[md@vm-kvm5643-app ~]$ sort -n -t&apos;-&apos; -k 1 tmp | uniq -c 3 2010-10-01 1 2016-04-30 1 2018-01-21 2 2019-02-28 找出文件中的重复行 123[md@vm-kvm5643-app ~]$ sort -n -t&apos;-&apos; -k 1 tmp | uniq -d2010-10-012019-02-28]]></content>
      <categories>
        <category>shell</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[测试笔记]]></title>
    <url>%2F2019%2F07%2F28%2F%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[从登录场景来看软件测试：1.验证功能性需求 显示功能需求：实现软件本身的功能需求 隐式功能需求：软件出现异常的处理机制是否正确 2.非功能性需求 安全：如密码加密，传输过程加密，SQL注入，XSS攻击（补充：拿到加密密码反解成本是否符合要求） 性能：响应时间是否达到要求，死锁和不合理资源等待情况，高并发情况下服务端是否存在内存泄漏 兼容性：不同浏览器，不同主机，不同网络，不同分辨率 单元测试： 第一，代码的基本特征和产生错误的原因（要做到代码功能逻辑正确，必须做到分类正确且完备没有遗漏，同事每个分类的逻辑处理正确）代码中的功能点相当于单元测试的‘等价类’ 第二，单元测试用例（单元测试是一个包含‘输入数据’和‘预计输出’的集合）输入数据并不只是被测试函数的输入参数，以下为一些分类： 1. 被测函数的输入参数 2. 被测函数内部需要读取的全局静态变量 3. 函数内部调用子函数获得的数据等等 ... 预计输出并不只是被测函数的返回值，以下为一些分类： 1. 被测函数的返回值 2. 被测函数的输出参数 3. 被测函数中进行的文件更新 4. 被测函数中进行的数据库更新等等 ... 第三，驱动代码，桩代码和Mock代码： 驱动代码指调用被测函数的代码，包含调用被测函数前的数据准备，调用被测函数和验证相关结果，驱动代码结构通常由单元测试框架决定 桩代码指用来代替函数内部某个尚未实现的函数的代码，与真是代码相比，桩代码具有与原函数相同的原形，只是内部实现不同，桩代码只是起到隔离 和补齐的作用，是被测代码能够独立编译，链接并独立运行，同时桩代码还有控制被测函数执行路径的作用，通常桩代码的验证(assert)逻辑出现在 驱动代码中 Mock代码：与桩代码类似，对于结果的验证通常出现在Mock代码中单元测试自动化的过程： 测试用例自动执行 用例框架代码生成的自动化：例如unittest框架提高开发效率 部分测试输入数据的自动化生成：例如通过条件判断语句来控制测试输入数据的自动化生成 自动桩代码的生成： 指自动化工具对被测代码进行分析扫描，自动为被测函数内部调用的其他函数生成可编程的桩代码 抽桩：在代码集成测试阶段，希望不再调用桩代码而调用真实代码 被测代码的自动化静态分析：主要指代码的静态扫描，目的是识别违反编码规则和编码风格的代码行，比较常见的工具有sonar和coverity。 测试覆盖率的自动统计与分析 集成测试：如单元测试最大的区别在于代码集成测试不允许使用桩代码web service测试 测试脚手架代码的自动化生成：例如httprunner可以自动生成项目脚手架 部分测试数据的自动化生成：与单元测试的区别在于单元测试的输入数据是函数的参数组合，API测试对应的是API的参数和API调用的payload response验证的自动化 基于测试工具(如postman)的自动化脚本生成:在使用测试工具测试时，在已经存在多个测试用例的情况下来基于代码实现API测试时，存在一个问题就在于，可以开发一个代码转换工具，自动将已有的测试用例(如JSON格式)转换为可执行代码，为后续CI/CD直接使用 测试计划： 测试范围：测什么和不测什么 测试策略：先测什么，后测什么，如何来测，采用什么样的测试类型和测试方法 测试资源：明确谁来测和在哪里测 测试进度：主要描述各类测试的开始时间，所需工作量，预计完成时间，并以此为依据来建议产品的上线发布时间 测试风险预估 需要掌握的非测试包含： 网站架构的核心知识：比如缓存，中间件，数据库等 容器技术：docker 云计算技术：配合docker使用 devops思维：以jenkins为例，到熟练应用和组合各种plugin来完成搭建灵活高效的流水线 前端开发技术：node.js pageobject模型的核心理念是：以页面(web page或native app page)为单位封装页面上的控件以及控件的部分操作，而测试用例，更确切的是说操作函数，基于页面封装对象来完成具体的操作，最典型的模式是XXXPage.YYYComponent.ZZZOperation GUI自动化的测试数据准备：对于测试的准备数据通常有三种方法：(通常最佳的选择是利用API创建数据，然后再调用数据库来达到特定的要求，实际情况可能存在混合使用) API调用：在实际项目中，通常会把这些API调用以代码的形式封装为测试数据工具 数据库操作：在数据的创建没有对外暴露接口或者创建成本过大时，可利用数据库操作来完成，由于数据库的修改可能涉及多张表这就导致了测试数据有可能会出问题，而且在实际项目中，经常出现SQL语句更新不及时而导致测试数据错误的问题，而且有可能错误还比较隐蔽，所以需要测试数据工具的版本管理 综合运用API调用和数据库操作 GUI测试数据自动生成： 根据GUI输入数据类型，以及对应的自定义的规则库自动生成测试输入数据，例如，输入框类型是string，基于规则库自动生成如NULL,SQL注入，超长字符串，数字字符等类型 对于需要组合多个测试输入数据的场景，测试数据自动生成可以完成多个测试数据的笛卡尔组合积，然后再以人工方式剔除非法的数据组合 GUI测试稳定性问题： 非预计弹出对话框：再测试过程中，弹出预期之外的对话框，处理方法为设计异常场景恢复模式 页面控件属性的细微变化：页面属性是有可能会变的，可以采用‘组合属性’来提高定位的精确度，如果在此基础上假如模糊匹配，将进一步提高控件的识别率目前QTP已实现模糊匹配，开源GUI框架还不支持，所以需要实现二次开发，思路是实现自己的对象识别控制层，也就是再原本对象识别基于上再额外封装一层，在这个额外封装层中实现模糊匹配的查询逻辑 被测系统的A/B(App/Web)测试：在测试脚本内部做分支处理，脚本能区别A/B两个版本并作出对应实现 随机的页面延迟导致控件识别失败：方法是可以假如重试机制，默认情况下是步骤级别的，也可以是页面级别或者业务级别的，这需要二次开发 测试数据问题 GUI自动化测试报告： 可以通过截图高亮显示操作步骤，对于全截图还是错误部分才截图可以设置一个autoScreenShot参数，为True时全截图，为False时部分截图，作为配置文件 相关截图函数可以在相关的Hook操作中使用或者在每个函数中使用，可根据具体业务逻辑选择 复杂场景下的API测试： 被测业务是由多个API调用协作完成：单一的前端操作可能出发多个API操作，必须从后端模拟前端接口的调用顺序；其次接口间存在数据依赖的问题后一个API根据前一个的返回来确定调用哪个API接口；此外，如何高效的获取单个前端操作所触发的API调用序列，核心思想为通过网络监控来捕捉前端操作所出发的API调用序列，可通过抓包工具或者通过大数据分析来获取序列 API测试过程的第三方依赖：API之间时存在相互依赖关系的，比如A调用B接口，但B接口还未完成，可通过Mock Server实现 异步API测试：异步API是指调用后立即返回，但实际工作并未完成，而是需要稍后查询或者回调API。对异步API的调用分两个方面，测试异步调用是否成功，和测试异步调用的业务逻辑处理是否正确。测试异步调用可通过检查返回值和后台工作线程是否被调用查看，业务逻辑部分通常需要查看数据库和消息队列中的值 自动生成API测试代码： 可以自研一个将postman输出的json文件自动转换为测试代码的工具，同时还会将测试的断言一并转换为代码 对于复杂的测试场景，比如顺序调用多个API，可以组装工具得到的测试用例代码 response结果发生变化时自动识别： 原因是因为API的向后兼容性，当向后兼容的API发生不一致的时候会发出告警 在API测试框架中引入内建数据库，推荐使用非关系型数据库(如mongodb)，然后再数据库中记录每次request和response的组合，而对于一些像时间戳的字段可以设置一个白名单过滤 微服务下的API测试： 单体架构与微服务架构：传统模式下的开发模式是单体架构，就是将所有业务场景的表示层，业务逻辑层和数据访问层都放在一个工程中，最终编译，打包，并部署再服务器上，工程大了之后容易导致难以维护且臃肿，微服务架构中一个复杂系统不再是由单体组成，可以拆分为多个服务，各个微服务运行在自己的进程中开发和部署没有依赖，甚至可以采用不同的编程语言。 解决微服务架构下的API测试：服务多了可能导致接口耦合变多，从而导致测试用例庞大，解决这个问题的核心是解决耦合性，可以使用基于消费者契约的API测试核心是只测试那些被真正实际使用到的API调用，那些没有使用到的API就不去测试。其次多服务模式下可能导致Mock Server使用泛滥，解决方法是，以之前的契约为依据，契约一般为json格式数据，我们可以以这个json文件为mock server依据使用 - Q：在基于消费者契约的API测试中，如果新开发的API或者加了新功能的API，由于之前没有实际的消费者，所欲无法通过gateway方法获得契约，应该如何操作 - A：需要两部分结合起来，老的功能走契约测试，新的功能和api继续沿用老的方法 并发数： 业务层面的并发数：指的是使用系统的用户总数 服务层面的并发数：指的是像服务器发起请求的数量 响应时间： 前端响应时间：取决于客户端收到服务端返回数据后完成渲染所消耗的时间，这与用户主观感受有关，所以不好定义 服务器响应时间：可以细化分为web服务器时间，应用服务器时间，数据库时间，以及各服务器之间通信的时间 系统吞吐量： 所有的吞吐量都必须以单位时间为前提，一般来说分为&apos;request/second&apos;,&apos;pages/second&apos;,&apos;bytes/second&apos;三种&apos;pages/second&apos;和&apos;bytes/second&apos;表示的吞吐量，主要受网络设置，服务器架构，应用服务器制约 &apos;request/second&apos;表示的吞吐量，主要受应用服务器和应用本身的限制 需要注意的是，虽说吞吐量可以反应服务器承受负载的情况，但在不同的并发用户数下，即使系统有相近的吞吐量，得到的系统性能也会不同 常用的7种性能测试： 后端性能测试：设计模式包含两种，基于性能需求目标的测试验证，和探索系统容量，并验证系统容量的可扩展性 前端性能测试：通常来说前端性能测试关注的是浏览器端的页面渲染时间，资源加载顺序，请求数量，前端缓存使用情况，资源压缩等。希望借此找到页面加载过程中比较耗时的操作和资源，然后进行针对性的优化。目前主流的是使用雅虎总结的7大类35条规则 代码级别性能测试：指在单元测试阶段就对代码的时间性能和空间性能进行评估，通常做法是对于单元测试用例执行n次，n通常是2000-5000，再取平均值，如果达到秒级，那么单元测试的实现逻辑通常需要优化 压力测试：验证系统临界饱和阶段的稳定性及性能指标 配置测试 并发测试：通常采用‘集合点并发’，比如要求的并发数为100，需要前99个用户到集合点前等待，知道最后一个用户到达时再同时发起请求，通常实际项目中，设置的并发数会比要求的并发数稍大 可靠性测试：本质是通过长时间模拟真实的系统负载来发现系统潜在的内存泄漏，链池回收等问题 性能测试的困难点： 性能需求的明确定义： 比如：医院体检需要每天支持完成8000个体检，这里需要明确是24小时还是每天工作时间，其次是由于业务原因，医院体检的高峰期时早上，所以基于2/8原则， 因该是在96分钟内完成6400人次的体检，通常会设置20%的冗余 测试结果的分析和性能问题的定位 测试数据准备的问题： On-the-fly(实时创建)模式：可以有效避免脏数据的问题，但是在微服务架构下很可能存在两个服务的不可用性，还有这种模式创建比较耗时，测试数据本身也存在复杂性问题，通常情况下，使用在易变数据的创建 Out-of-box(事先创建数据)模式：最大的问题就是脏数据的处理，有可能创建好的数据被其他人使用了而带来脏数据或者用例执行会失败，通常情况下，使用在稳定数据的创建 数据准备1.0：这种模式最典型的模式就是将索要准备的数据封装为准备函数，这种可以是基于API的，可以是基于数据库的，还可以是相结合的，这种模式带来的问题就是包含大量参数的情况下会导致组合的复杂性，解决办法是分层封装，通过调用底层接口为上层接口提供支持，但是也会带来新的问题，就是对于参数较多的情况会带来封装过于复杂的情况，还有就是底层包发生变化时，需要修改所有的调用函数，还有可能会导致数据准备函数的jar包版本升级过于频繁这也就带来的数据准备2.0 数据准备2.0：引入Builder Pattern数据准备方式，其实就是设置默认值，这样就不会带来底层发生变化上层会频繁修改的问题，其次引入Builder Strategy模式，这是为了处理以下四种情况， (1)处理已有的数据，只需搜索就可用 (2)需要创建数据 (3)有数据就使用已有的，没有就创建数据 (4)使用OUT-OF-BOX模式但是也有新问题，跨语言平台的使用 数据准备3.0：为了解决跨平台问题使用RESTFUL API接口，其次，为了自动化，引入core service和一个内部数据库，内部数据库用来存放测试用的元数据，core service在内部数据库的支持下，提供数据质量和数量的管理，原理是，当一个测试数据被创建成功后，为了使得下次调用更加高效，会自动创建一个Jenkins job，这个Jenkins job会自动创建100条同类型的数据，同时将数据保存到数据库中，当数据库中剩余的同类型数据低于20条时，对应的Jenkins job会自动补全到100条 后端高性能服务架构： 缓存：缓存可以扩展系统性能，有降低后端负载的作用，系统和软件对应不同层级的缓存 浏览器级别的缓存，会用来存储之前网络上下载的静态资源 CDN本质也是缓存，属于部署在网络服务供应商机房中的缓存 反向代理服务器也是缓存，属于用户数据中心最前端的缓存 数据库中的热点数据，在应用服务器集群中有一级缓存，在缓存服务器集群中有二级缓存 甚至在DNS服务器上也有缓存 采用缓存的原因在于二八原则，即80%的数据访问集中在20%的数据上 分布式缓存架构主流包含以下两种： 1. JBoss Cache：在所有需要缓存的机器都同步所有缓存的副本，这就导致同步的代价比较高，但是速度快 所以比较适用于规模不是很大的缓存集群 2. Memcached：集群中每个节点缓存的数据都不一样，缓存的使用者基于hash算法，所以导致速度相比较慢 但是由于缓存容量大，存储效率高而成为主流缓存的代名词对于测试人员的角度看待相关缓存测试： 1. 对于前端测试场景，需要分别考虑缓存命中和缓存不命中时的页面加载情况 2. 基于缓存过期测试策略的设计，需要考虑到必须要重新获取数据的测试场景 3. 需要针对可能存在的脏数据，进行针对性测试。缓存脏数据是指数据库中已经更新了，但是缓存中的数据还没有更新 4. 需要针对可能存在的缓存穿透进行必要的测试。缓存穿透是指访问的数据不存在，也就永远不会有被缓存的机会，会一直对数据库发起请求 5. 系统冷启动时，在缓存预热阶段的数据库访问压力是否会超过数据库实际承载能力 6. 对于分布式缓存集群的扩容来说，由于不同的缓存集群可能采取不同的算法，扩容对集群的影响也不同，所以需要进行必要的性能评估和测试网站高可用架构设计： 在硬件层面上加入必要的冗余，来保证一台或多台服务器发生故障时，网站的可用性，必要时引入集群。 对于测试人员来说，知道了应用服务器集群的工作原理，在设计测试用例时，可以针对单个节点或多个节点故障时的情况 灰度发布：前提必须保证服务器采用了集群架构，以下为假定包含100个节点的集群中升级应用版本的灰度发布 1. 首先，从负载均衡器服务器列表中删除一个节点 2. 然后，将新版本的引用部署到这台被删除节点的服务器上，并重启服务 3. 重启完成后，将这台服务器重新挂载到负载均衡服务器上，使其接受外部流量并密切观察 4. 如果没有问题将会重复步骤升级剩余节点，如果有问题会回滚上一个版本 加强应用上线前测试或开启预发布环境 网站的可伸缩性： 应用服务器的可伸缩性：对于测试人员来讲，可从以下几点入手测试角度 1. 需要通过压力测试来得出单一节点的负载承受能力 2. 验证系统的整体承受能力，是否随集群的节点数量呈线性增长 3. 集群中节点数量是否有上线 4. 新加入的节点是否可以提供与原来节点无差异的服务 5. 对于有状态的应用，是否能够实现一次会话的多次请求都被分配到一台服务器上 6. 验证负载均衡算法的准确性 缓存集群的可伸缩性：对于测试人员来讲，以下为建议点 1. 针对缓存集群中新增的节点的测试，验证其对原有缓存的影响足够小 2. 验证系统冷启动的时候，缓存中还没有任何数据时，如果此时网站负载较大，数据库是否能承受这样的压力 3. 需要验证各种情况下，缓存数据与数据库数据的一致性 4. 验证是否对已经潜在的缓存穿透攻击进行了处理 数据库的可伸缩性：对于测试人员来讲，可从以下角度考虑 1. 正确读取刚写入数据的延迟时间 ...网站的可扩展性： 消息队列：分布式消息队列采用的时生产者与消费者模式，消息的发送者就是生产者，接收者就是消费者.引入了消息队列之后，提高了系统的可扩展性。 1. 从性能上看，消息发送者不用等到接收者实际处理完才返回，也就是从原来的同步处理变成了异步处理 2. 如果消息接收者模块发生了短时间故障，此时并不影响消息发送者往消息队列中发送数据，等接收者故障恢复之后可继续处理 3. 消息队列的核心时一个无状态存储，所以当消息队列内存不够用时，可简单增加存储空间即可测试人员在测试包含消息队列情况时，需要关注的点： 1. 从构造测试数据来看，为了以解耦的方式测试系统的各个模块，需要在消息队列中构造测试数据，这也是 很多自动化测试框架会集成消息队列写入工具的主要原因 2. 还要验证消息队列的读取数据是否正确，所以自动化测试框架也会配备消息队列的读取工具 3. 还需要考虑消息队列队列满和消息队列扩容情况下的测试]]></content>
      <categories>
        <category>测试</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Python常见问题解析]]></title>
    <url>%2F2019%2F07%2F28%2Fpython%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[解析不定长的iterable： _表示占位符，但只能占一个位置，超过一个位置会抛出ValueError *表示不定长的占位，同时会将不定长的数据放入一个列表中,与切片相比的优势在于可以解析不定长的iterable,当然，_和也能配合`_`使用，表示丢弃掉不定长的iterable123456&gt;&gt;&gt; a = (1, 21, 3, 4, 5, 6)&gt;&gt;&gt; b, *c, _, _ = a&gt;&gt;&gt; print(b)1&gt;&gt;&gt; print(c)[21, 3, 4] 队列与列表的区别： 队列的两端插入和删除的时间复杂度为O(1),列表的两端插入和删除的时间复杂度为O(N) 列表为栈结构，为先进后出，队列为先进先出，列表不限长度，队列默认也为不限长度，但是可以使用maxlen来指定队列长度，超过长度时再append数据时，会先弹出队列第一个数据，再append进队列最后一个数据 队列的相关接口： append appendleft pop popleft extend insert123456789101112131415from collections import deque&gt;&gt;&gt; a = deque(maxlen=3)&gt;&gt;&gt; a.append(1)&gt;&gt;&gt; a.append(2)&gt;&gt;&gt; a.append(3)&gt;&gt;&gt; a.append(4)&gt;&gt;&gt; for i in a:... print(i)...234&gt;&gt;&gt; a[0] = 10&gt;&gt;&gt; print(a)deque([10, 3, 4], maxlen=3) 提取出iterable中的N个最大数或最小数 可用heapq中的nlargest或nsmallest 也可用sorted(iterable, key=key, reverse=True)[:n]或sorted(iterable, key=key, reverse=True)[n:] 处理复杂字典类型12345d = &#123;&#125;for key, value in pairs: if key not in d: d[key] = [] d[key].append(value) 相等 1234from collections import defaultdictd = defaultdict(list)for key, value in pairs: d[key].append(value) 字典排序由于字典是无序的结构，想要控制字典中的顺序，可以使用collections模块中的OrderedDict类，OrderedDict 内部维护着一个根据键插入顺序排序的双向链表，这会在每次插入新元素的时候，会被放入链表尾部。对于一个已经存在的键的重复赋值不会改变键的顺序。ps:值得注意的是，OrderedDict的空间消耗是普通字典类型的两倍 12345678910from collections import OrderedDictimport jsond = OrderedDict()d['foo'] = 1 d['bar'] = 2 d['spam'] = 3 d['grok'] = 4 print(json.dumps(d))# output:&#123;"foo": 1, "bar": 2, "spam": 3, "grok": 4&#125; OrderedDict相关接口： popitem move_to_end copy keys values items zipzip可与字典配合使用用于根据key或者value排序等等 1234567prices = &#123; 'ACME': 45.23, 'AAPL': 612.78, 'IBM': 205.55, 'HPQ': 37.20, 'FB': 10.75 &#125;print(max(zip(prices.values(), prices.keys()))) 值得注意的是：zip()函数创建的是一个只能访问一次的迭代器 123prices_and_names = zip(prices.values(), prices.keys())print(min(prices_and_names)) # OKprint(max(prices_and_names)) # ValueError: max() arg is an empty sequence 字典的key,value和item字典的 keys() 方法返回一个展现键集合的键视图对象。 键视图的一个很少被了解的特性就是它们也支持集合操作，比如集合并、交、差运算。 所以，如果你想对集合的键执行一些普通的集合操作，可以直接使用键视图对象而不用先将它们转换成一个 set。 字典的 items() 方法返回一个包含 (键，值) 对的元素视图对象。 这个对象同样也支持集合操作，并且可以被用来查找两个字典有哪些相同的键值对。 字典的 values() 方法也是类似，但是它并不支持这里介绍的集合操作。 某种程度上是因为值视图不能保证所有的值互不相同 对于重复数据的思考set虽然能很好且很快的取出重复数据，但是带来的问题是，由于set中的元素是无序的，会导致set去重之后的序列也会有被打乱的风险，在此提供的一个思路是，使用生成器函数结合set来去重,且当序列中的元素为unhashable时，同样适用。此方法不仅仅适用于处理普通序列，且可以处理文件中消除重复行等操作。 1234567def dedupe(items, key=None): seen = set() for item in items: val = item if key is None else key(item) if val not in seen: yield item seen.add(val) 对于iterable中的计数问题想要得到iterable中出现次数最多的元素的时候，可以采用sort之后手动计数的方法，更为简单的方式是通过collections模块中的Counter函数。 123456from collections import Counterwords = ( 'look', 'into', 'my', 'eyes', 'look', 'into', 'my', 'eyes','the', 'eyes', 'the', 'eyes', 'the', 'eyes', 'not', 'around', 'the','eyes', "don't", 'look', 'around', 'the', 'eyes', 'look', 'into','my', 'eyes', "you're", 'under' )count = Counter(words)print(count.most_common(3)) # output:[('eyes', 8), ('the', 5), ('look', 4)] Counter提供的常见接口： most_common update 值得注意的是，Counter支持常见的算术运算，包含+,-,and,or等 字典列表排序问题优化在此引入operator模块中的itemgetter函数，f = itemgetter(2)时，调用f(r)返回r[2]，当然，也可以传入多个参数，返回为一个包含多个下标的元祖。对于排序的影响在于，sorted函数排序时，可以指定按照指定的值排序。例如： 1rows_by_fname = sorted(rows, key=itemgetter('fname')) 这表明会按照fname来排序，当然也可以传入多个参数来排序，但是有时候也可以使用lambda 来代替 1rows_by_fname = sorted(rows, key=lambda r: r['fname']) 相比而言，itemgetter方式会更快一些 排序不支持原生比较的对象如果需要排序的是一个实例序列(类似于[User(3), User(5), User(6)]),想通过实例的属性来进行排序时，可使用operator模块中的attrgetter函数,例如: 123456789101112from operator import attrgetterclass User: def __init__(self, user_id): self.user_id = user_id def __repr__(self): return 'User(&#123;&#125;)'.format(self.user_id)def sort_notcompare(): users = [User(23), User(3), User(99)] print(sorted(users, key=attrgetter('user_id'))) 当然也可以通过lambda排序，但lambda排序会慢一点 1sorted(users, key=lambda u: u.user_id) 对于attrgetter函数，提供了如下接口: f = attrgetter(&#39;name&#39;),调用f(r)时，返回f.name f = attrgetter(&#39;name&#39;,&#39;date&#39;)，调用f(r)时，返回`(f.name,f.date)`` f = attrgetter(&#39;name.first&#39;,&#39;name.last&#39;)，调用f(r)时，返回(f.name.first,f.name.last) 字典列表的分组排序当需要对一个字典或实例的列表采用分组排序时，在此引入itertools模块中的groupby函数，作用类似于MYSQL中的group by函数，使用此函数的前提在于，需要进行分组排序的列表，必须提前根据需要分组的元素进行了排序。以下为示例： 123456789101112131415161718192021222324252627282930from operator import itemgetterfrom itertools import groupbyrows = [ &#123;'address': '5412 N CLARK', 'date': '07/01/2012', 'name': 'jack'&#125;,&#123;'address': '5148 N CLARK', 'date': '07/04/2012', 'name': 'mary'&#125;,&#123;'address': '5800 E 58TH', 'date': '07/02/2012', 'name': 'tom'&#125;,&#123;'address': '2122 N CLARK', 'date': '07/03/2012', 'name': 'bob'&#125;,&#123;'address': '5645 N RAVENSWOOD', 'date': '07/02/2012', 'name': 'jay'&#125;,&#123;'address': '1060 W ADDISON', 'date': '07/02/2012', 'name': 'peter'&#125;, &#123;'address': '4801 N BROADWAY', 'date': '07/01/2012', 'name': 'jack'&#125;, &#123;'address': '1039 W GRANVILLE', 'date': '07/04/2012', 'name': 'jan'&#125;,]# Sort by the desired field firstrows.sort(key=itemgetter('date'))for date, items in groupby(rows, key=itemgetter('date')): print(date) for i in items: print(' ', i)#output'''07/01/2012 &#123;'address': '5412 N CLARK', 'date': '07/01/2012', 'name': 'jack'&#125; &#123;'address': '4801 N BROADWAY', 'date': '07/01/2012', 'name': 'jack'&#125;07/02/2012 &#123;'address': '5800 E 58TH', 'date': '07/02/2012', 'name': 'tom'&#125; &#123;'address': '5645 N RAVENSWOOD', 'date': '07/02/2012', 'name': 'jay'&#125; &#123;'address': '1060 W ADDISON', 'date': '07/02/2012', 'name': 'peter'&#125;07/03/2012 &#123;'address': '2122 N CLARK', 'date': '07/03/2012', 'name': 'bob'&#125;07/04/2012 &#123;'address': '5148 N CLARK', 'date': '07/04/2012', 'name': 'mary'&#125; &#123;'address': '1039 W GRANVILLE', 'date': '07/04/2012', 'name': 'jan'&#125;''' 过滤列表中的元素提供以下几种思路 使用列表生成式，优点在于代码量小，缺点在于过滤复杂条件麻烦且不宜读，而且会占用大量内存 使用filter函数，返回一个迭代器，而且可以将复杂的过滤条件封装在一个函数中 使用itertools模块中的compress函数，也返回一个迭代器，而且采用的是一个Boolean列表来过滤另外一个列表 列表生成式过滤数据: 12&gt;&gt;&gt; mylist = [1, 4, -5, 10, -7, 2, 3, -1]&gt;&gt;&gt; [n for n in mylist if n &gt; 0] filter函数： 123456789values = ['1', '2', '-3', '-', '4', 'N/A', '5']def is_int(val): try: x = int(val) return True except ValueError: return Falseivals = list(filter(is_int, values))print(ivals) compress函数： 1234567891011addresses = [ '5412 N CLARK', '5148 N CLARK', '5800 E 58TH', '2122 N CLARK', '5645 N RAVENSWOOD', '1060 W ADDISON', '4801 N BROADWAY', '1039 W GRANVILLE',]counts = [ 0, 3, 10, 4, 1, 7, 6, 1] 123456&gt;&gt;&gt; from itertools import compress&gt;&gt;&gt; more5 = [n &gt; 5 for n in counts]&gt;&gt;&gt; more5[False, False, True, False, False, True, True, False]&gt;&gt;&gt; list(compress(addresses, more5))['5800 E 58TH', '1060 W ADDISON', '4801 N BROADWAY'] 过滤字典可采用字典推导式 合并多个字典 可采用dict中的update来讲两个字典合并为一个字典 可使用collections模块中的ChainMap函数，而且该函数只会创建一个临时的合并字典，以供数据采用，所以效率会更高一点 12a = &#123;'x': 1, 'z': 3 &#125;b = &#123;'y': 2, 'z': 4 &#125; 1234from collections import ChainMapc = ChainMap(a,b)print(c['x']) # Outputs 1 (from a)print(c['y']) # Outputs 2 (from b) ChainMap提供以下接口: new_child新加入一个字典进入一个ChainMap parents返回父节点 maps返回字典列表形式 如果出现重复键，那么第一次出现的映射值会被返回 字符串分割问题 大多数情况可以使用str.splite满足要求 更好的选择可以是使用re模块的splite函数 1234&gt;&gt;&gt; line = 'asdf fjdk; afed, fjek,asdf, foo'&gt;&gt;&gt; import re&gt;&gt;&gt; re.split(r'[;,\s]\s*', line)['asdf', 'fjdk', 'afed', 'fjek', 'asdf', 'foo'] 值得注意的是，需要选择是否使用括号来捕捉分组。如果又想使用括号，并且不想捕捉括号内分组，可使用?:...模式 12&gt;&gt;&gt; re.split(r'(?:,|;|\s)\s*', line)['asdf', 'fjdk', 'afed', 'fjek', 'asdf', 'foo'] re.splite提供了maxsplite参数，默认为0，表示将字符串中所有值都切分，如果不为0，表示将返回包含给定个数字符串的列表，剩余元素将作为最后一个元素输出 12&gt;&gt;&gt; print(re.split('(?:\s|,|;)\s*', line, 3))['asdf', 'fjdk', 'afed', 'fjek,asdf, foo'] 关于字符串开头或结尾的思考str提供了startwsitch和endswitch函数来检查字符串的开头或结尾，值得注意的是，传参只能为str类型或tuple类型。当然，参数也支持start和end，作为开始和结束位置的检查点 对于开头或结尾的处理，使用切片或者是正则表达式也是可以的，但是使用startwsitch和endswitch会更快且更方便 123&gt;&gt;&gt; url = 'http://www.python.org'&gt;&gt;&gt; print(url.startswith('www', 7)) True 关于字符串替换 对于一般情况，可使用str.replace()即可满足 更多情况，需要使用re模块的sub函数，来个性化定制需要替换的值 str提供了translate()来替换和清理较为复杂的字符串 1234&gt;&gt;&gt; text = 'Today is 11/27/2012. PyCon starts 3/13/2013.'&gt;&gt;&gt; import re&gt;&gt;&gt; re.sub(r'(\d+)/(\d+)/(\d+)', r'\3-\1-\2', text)'Today is 2012-11-27. PyCon starts 2013-3-13.' 反斜杠数字比如 \3 指向前面模式的捕获组号。如果需要使用相同模式来做多次替换，可以考虑先编译来提升性能 而对于更加复杂的替换，可以使用一个回调函数来替代 1234567&gt;&gt;&gt; from calendar import month_abbr&gt;&gt;&gt; def change_date(m):... mon_name = month_abbr[int(m.group(1))]... return '&#123;&#125; &#123;&#125; &#123;&#125;'.format(m.group(2), mon_name, m.group(3))...&gt;&gt;&gt; datepat.sub(change_date, text)'Today is 27 Nov 2012. PyCon starts 13 Mar 2013.' 如果想要查看发生了多少次替换，可用re.subn()来代替 12345&gt;&gt;&gt; newtext, n = datepat.subn(r'\3-\1-\2', text)&gt;&gt;&gt; newtext'Today is 2012-11-27. PyCon starts 2013-3-13.'&gt;&gt;&gt; n2 ps:对于大多数正则表达式来说，都提供一个flags参数，flags=re.IGNORECASE是，表示忽略大小写 多行匹配模式对于正则表达式来说，点(.)并不支持换行符匹配，对于需要多行匹配的问题，提供两种思路： 在正则表达式中加入换行匹配规则，例：re.compile(r&#39;/\*((?:.|\n)*?)\*/&#39;) re.compile()函数接受一个参数叫re.DOTALL,表示点(.)接受包括换行符在内的任意字符 123&gt;&gt;&gt; comment = re.compile(r'/\*(.*?)\*/', re.DOTALL)&gt;&gt;&gt; comment.findall(text2)[' this is a\n multiline comment '] 字符串对齐 str提供了ljust() , rjust() 和 center() 方法可以指定对齐方式 使用foemat()函数对齐 123456&gt;&gt;&gt; format(text, '&gt;20')' Hello World'&gt;&gt;&gt; format(text, '&lt;20')'Hello World '&gt;&gt;&gt; format(text, '^20')' Hello World ' 填充字符 12&gt;&gt;&gt; format(text, '=&gt;20s')'=========Hello World' 格式化多个值 12&gt;&gt;&gt; '&#123;:&gt;10s&#125; &#123;:&gt;10s&#125;'.format('Hello', 'World')' Hello World' 关于字符串拼接的思考 最主流的还是使用str提供的join函数 对于简单的拼接也可使用+和format()函数 对于性能的考虑 使用+连接符去操作大量字符串的效率非常低下，因为加号会带来内存复制和垃圾回收，应尽量避免写下面的函数 123s = ''for p in parts: s += p 更为聪明的做法是使用生成器表达式 123&gt;&gt;&gt; data = ['ACME', 50, 91.1]&gt;&gt;&gt; ','.join(str(d) for d in data)'ACME,50,91.1' 同时，注意不必要的字符连接 123print(a + ':' + b + ':' + c) # Uglyprint(':'.join([a, b, c])) # Still uglyprint(a, b, c, sep=':') # Better 关于字节类型的字符串(byte类型) 字节类型的字符串支持大部分字符串的操作，例如：replace,splite,切片,正则表达式等 值得注意的是，字节类型的字符串通过索引返回的是整数而非操作数，例如 123&gt;&gt;&gt; a = b'hello'&gt;&gt;&gt; print(a[0])104 ps:与python2对比，字节类型的字符串与普通字符串并无区别，例如： 12345&gt;&gt;&gt; print type(bytes('hello'))&lt;type 'str'&gt;&gt;&gt;&gt; a = b'hello'&gt;&gt;&gt; print a[0]h 字节类型与字符串类型的相互转换： 123456&gt;&gt;&gt; a = b'hello'&gt;&gt;&gt; b = a.decode('ascii')&gt;&gt;&gt; print(b)hello&gt;&gt;&gt; print(b.encode('ascii'))b'hello' 关于数字的处理四舍五入 一般情况，可用python内置函数round(value, ndigits)即可,ndigits表示对于小数点后几位来四舍五入，当然，也可为负数，表示对整数后几位进行四舍五入 浮点数精度问题 123456&gt;&gt;&gt; a = 4.2&gt;&gt;&gt; b = 2.1&gt;&gt;&gt; print(a+b)6.300000000000001&gt;&gt;&gt; print((a+b) == 6.3)False 这些错误是由底层CPU和IEEE 754标准通过自己的浮点单位去执行算术时的特征.所以没法自己去避免这些误差 想要无误差处理浮点数精度问题，可参考decimal模块Decimal函数 1234from decimal import Decimala = Decimal('4.2')b = Decimal('2')print(a+b) #output:6.2 随机选择 random.choice()表示随机选择一个字符 1234&gt;&gt;&gt; import random&gt;&gt;&gt; values = [1, 2, 3, 4, 5, 6]&gt;&gt;&gt; random.choice(values)2 random.sample()表示随机选择n个字符 12&gt;&gt;&gt; random.sample(values, 2)[6, 2] random.shuffle()表示打乱排序 123&gt;&gt;&gt; random.shuffle(values)&gt;&gt;&gt; values[2, 4, 6, 5, 3, 1] random.randint() 表示随机选择一个整数 12&gt;&gt;&gt; random.randint(0,10)2 random.random() 表示随机生成一个从0到1的浮点数 12&gt;&gt;&gt; random.random()0.9406677561675867 random.getrandbits(k) 表示随机生成k为二进制随机数的整数 12&gt;&gt;&gt; random.getrandbits(10)512 PS:值得注意的是，random模块采用的是Mersenne Twister 算法来计算生成随机数。这是一个确定性算法， 但是你可以通过 random.seed() 函数修改初始化种子。所以，对于安全性要求高的应尽量避免random模块的使用 时间与日期时间段 123456&gt;&gt;&gt; from datetime import timedelta&gt;&gt;&gt; a = timedelta(days=2, hours=6)&gt;&gt;&gt; print(a)2 days, 6:00:00&gt;&gt;&gt; print(a.days)2 值得注意的是，timedelta函数并没有提供年和月的时间段函数，使用timedelta的优势在于可是与datetime的时间做运算 1234&gt;&gt;&gt; from datetime import datetime&gt;&gt;&gt; a = datetime(2012, 9, 23)&gt;&gt;&gt; print(a + timedelta(days=10))2012-10-03 00:00:00 迭代器与生成器使用生成器实现深度优先算法 123456789101112131415161718192021222324252627282930313233class Node: def __init__(self, value): self._value = value self._children = [] def __repr__(self): return 'Node(&#123;!r&#125;)'.format(self._value) def add_child(self, node): self._children.append(node) def __iter__(self): return iter(self._children) def depth_first(self): yield self for c in self: yield from c.depth_first()# Exampleif __name__ == '__main__': root = Node(0) child1 = Node(1) child2 = Node(2) root.add_child(child1) root.add_child(child2) child1.add_child(Node(3)) child1.add_child(Node(4)) child2.add_child(Node(5)) for ch in root.depth_first(): print(ch) # Outputs Node(0), Node(1), Node(3), Node(4), Node(2), Node(5) 反向迭代 对于一个iterable可以使用reversed()函数来实现反向迭代，例如 1print(reversed([1, 2, 3])) 当然，列表的内存消耗过大，如果处理大文件形成的列表可以转换为元祖进行处理。 除此之外，可以使用自定义类中的魔法函数__reversed__()来实现反向迭代，例如 1234567891011121314151617class Countdown: def __init__(self, start): self.start = start # Forward iterator def __iter__(self): n = self.start while n &gt; 0: yield n n -= 1 # Reverse iterator def __reversed__(self): n = 1 while n &lt;= self.start: yield n n += 1 相比而言，反向迭代器运行非常高效，因为它不再需要将数据填充到一个列表中然后再去反向迭代这个列表。 生成器与迭代器切片 生成器与迭代器并不能像普通列表切片一样，可以通过itertools模块中的islice()函数实现切片，例 123456from itertools import islicea = iter([1, 2, 3, 4, 5])b = islice(a, 1, 2)for i in b: print(i)#output:2 值得注意的是，islice()是会消耗掉迭代器或生成器中的值，因为必须考虑到迭代器是一个不可逆的过程 123for i in a: print(i)#output:3, 4, 5 处理文件中需要跳过的部分 顾名思义，跳过不需要的部分，可使用itertools模块中的dropwhile()函数，例如，可跳过注释部分，代码如下 123with open(path, 'r') as f: for line in dropwhile(lambda line: line.startswith('#'), f): print(line, end='') 值得注意的是，dropwhile函数只能跳过文章开头的部分，并不能跳过中间满足条件的部分 如果想要提取文件中的部分值，仍可通过islice函数获取 123with open(path, 'r') as f: for line in islice(f, 2, 5): print(line, end='') 如果想要跳过所有满足条件的注释行，可改动下代码 1234with open(path, 'r') as f: lines = (line for line in f if not line.startswith('#')) for line in lines: print(line, end='') 同时迭代多个序列 简言之，就是将多可序列+起来，可使用内置函数zip()，zip()函数返回一个迭代器，并且迭代长度和参数中最短序列长度一致 12345a = [1, 2, 3]b = ['m', 'n']for i in zip(a, b): print(i)#output:(1, 'm'),(2, 'n') 当然，返回参数中最大长度也是可以的，可以使用itertools模块中的zip_longest函数 123456from itertools import zip_longesta = [1, 2, 3]b = ['m', 'n']for i in zip_longest(a, b, fillvalue=0): print(i)#output:(1, 'm'),(2, 'n'),(3, 0) 关于多个可迭代对象求和 1234a = [1, 2, 4]b = [5, 6, 7]for i in a+b: print(i) 通过这种常规的方式是可以达到目的的，有两个问题： 列表过大时，消耗内存也会太大 a,b不是同一种类型时，这种操作会抛异常 所以，在此引入itertools模块中的chain函数，上面的可改为： 12345from itertools import chaina = [1, 2, 4]b = [5, 6, 7]for i in chain(a, b): print(i) chain的优势在于，可处理两种不同类型的iterable，且会省内存 关于嵌套序列的处理 关于嵌套序列的处理存在多种方法，在此使用生成器的方式，在于节省内存且代码优雅，例如 12345678910111213from collections import Iterabledef flatten(items, ignore_types=(str, bytes)): for x in items: if isinstance(x, Iterable) and not isinstance(x, ignore_types): yield from flatten(x) else: yield xitems = [1, 2, [3, 4, [5, 6], 7], 8]# Produces 1 2 3 4 5 6 7 8for x in flatten(items): print(x) 文件与IO关于文件读取 open()函数打开文件时，对于换行符的识别在UNIX和Windows下的识别是不一样的(分别为\n和\r\n),默认情况下，python会统一处理换行符，并在输出时，将换行符替换为\n，当然也可以手动指定换行符，使用newline参数指定 在文件编码时，可能出现编码和解码方式不一样而导致打开文件失败的情况，可指定error参数，来处理打开失败的情况 123with open(path, 'rt', encoding='ascii', errors='replace') as f: print([f.read()])#output:['172.24.107.153\n������'] 123with open(path, 'rt', encoding='ascii', errors='ignore') as f: print([f.read()])#output:['172.24.107.153\n'] 关于print函数 1print(self, *args, sep=' ', end='\n', file=None) 其中seq是指定多个参数时的分隔符，默认为空格；end是在输出的末尾加上需要的字符，使用end参数在输出中禁止换行，默认为换行符；file是指流文件，可以用作重定向字符到文件中 1print(2019, 7, 24, sep='-', end='!!!') 值得注意的是，对于参数合并时，&#39;&#39;.join也支持字符串合并，但是仅支持字符串合并，而sep参数能将不同类型的参数合并在一起 处理文件不存在时才能写入 言下之意是，在目录中不能存在这个文件名，如果存在就会抛异常，通常会存在以下两种方案： 通过os.path.exists判断该文件是否存在 open函数提供了x参数，表示会创建一个文件，并以写的方式打开，且文件如果已经存在会抛异常 12with open(path, 'x') as f: f.write('write text') 值得注意的是，x参数是python3才引入的，之前版本并不支持 字符串I/O操作 在涉及到需要创建一个文件来存储数据时，通常存在以下两种方式： 在本地磁盘创建一个文件，以写的方式放入数据，用完之后再删了，这样的有点在于不会存在太大的内存限制，缺点在于可能会存在读写速度问题 使用io.StringIO()函数来处理，优势在于将数据存在内存中，读写速度会高于磁盘读写，缺点在于，数据量过大可能会带来内存问题 1234567891011121314# write a file&gt;&gt;&gt; from io import StringIO&gt;&gt;&gt;&gt;&gt;&gt; s = StringIO()&gt;&gt;&gt; s.write('这是一个文件io操作\n')11&gt;&gt;&gt; print('今天是星期四\n',file=s)&gt;&gt;&gt; print('明天星期五\n',end='',file=s)&gt;&gt;&gt; s.getvalue()'这是一个文件io操作\n今天是星期四\n\n明天星期五\n'# read a file&gt;&gt;&gt; s = StringIO('today\n')&gt;&gt;&gt; s.read(3)'tod' 同理，涉及到二进制数据时，需要用BytesIO函数来代替 需要注意的是， StringIO 和 BytesIO 实例并没有正确的整数类型的文件描述符。 因此，它们不能在那些需要使用真实的系统级文件如文件，管道或者是套接字的程序中使用。 固定字符串长度迭代文件 12345678from functools import partialRECORD_SIZE = 32with open('somefile.data', 'rb') as f: records = iter(partial(f.read, RECORD_SIZE), b'') for r in records: ... 这样会不断产生一个固定大小的数据块，当然也可以自己根据需要产生的数据块进行逻辑处理。 需要注意的地方在于，普通文本的处理方式，默认迭代方法为一行一行的读取，这通常是更普遍的做法 关于os.path处理文件路径 123456789101112131415161718&gt;&gt;&gt; import os&gt;&gt;&gt; path = '/Users/beazley/Data/data.csv'&gt;&gt;&gt; os.path.basename(path) # 获取文件名'data.csv'&gt;&gt;&gt; os.path.dirname(path) # 获取文件目录'/Users/beazley/Data'&gt;&gt;&gt; os.path.join('tmp', 'data', os.path.basename(path)) # 组合路径'tmp/data/data.csv'&gt;&gt;&gt; path = '~/Data/data.csv'&gt;&gt;&gt; os.path.expanduser(path) # 获取文件绝对路径'/Users/beazley/Data/data.csv'&gt;&gt;&gt; os.path.splitext(path) # 分离扩展名和路径('~/Data/data', '.csv') 123456789101112&gt;&gt;&gt; os.path.isfile('/etc/passwd')True&gt;&gt;&gt; os.path.isdir('/etc/passwd')False&gt;&gt;&gt; os.path.islink('/usr/local/bin/redis-sentinel')True&gt;&gt;&gt; os.path.realpath('/usr/local/bin/redis-sentinel')'/usr/local/bin/redis-server'&gt;&gt;&gt; os.path.getsize('/usr/local/bin/redis-sentinel')2109680&gt;&gt;&gt; os.path.getmtime('/usr/local/bin/redis-sentinel')1472713782.0 不同类型文件格式处理csv文件处理 读取csv文件 包含两种读取方式，可通过字符串读取和字典读取 12345import csvwith open('stocks.csv') as f: f_csv = csv.reader(f) headers = next(f_csv) for row in f_csv: 1234import csvwith open('stocks.csv') as f: f_csv = csv.DictReader(f) for row in f_csv: 写入csv文件 也包含两种写入方式，可通过普通字符串写入和字典写入 1234with open('stocks.csv','w') as f: f_csv = csv.writer(f) f_csv.writerow(headers) f_csv.writerows(rows) 1234with open('stocks.csv','w') as f: f_csv = csv.DictWriter(f, headers) f_csv.writeheader() f_csv.writerows(rows) csv文件中分割字符串 csv.reader()函数中带有delimiter参数可指定分割方式 123with open('stock.tsv') as f: f_tsv = csv.reader(f, delimiter='\t') for row in f_tsv: csv文件字符转换 由于csv中读取的数据都为字符串类型，只能手动操作数据类型转换 123456col_types = [str, float, str, str, float, int]with open('stocks.csv') as f: f_csv = csv.reader(f) headers = next(f_csv) for row in f_csv: row = tuple(convert(value) for convert, value in zip(col_types, row)) 当然这样存在风险性，是因为实际情况中csv文件或多或少存在缺失的数据，这样可能导致数据转换抛出异常 处理json数据 处理xml文件 python处理xml文件通常存在多种方式，这里分别以处理简单文件和大文件为例 处理一般文件，通常使用ElementTree模块,python3.3之后会自动寻找可用的C库来加快速度 1234try: import xml.etree.cElementTree as ETexcept ImportError: import xml.etree.ElementTree as ET 解析根节点 123tree = ET.parse('111.xml')root = tree.getroot()#&lt;Element data at 0x1f74dabef48&gt;, so root is data 而获取root的原因在于方便后面解析使用, 通常情况，xml结构标识为&lt;tag attrib1=1&gt;text&lt;/tag&gt;tail 1234567#source data:&lt;country name="Liechtenstein"&gt;&gt;&gt;&gt; root[0].attrib #获取属性&#123;'name': 'Liechtenstein'&#125;&gt;&gt;&gt; root[0][0].text #获取文本2&gt;&gt;&gt; root[0][0].tag #获取标签rank 当然，root也是可以迭代的 12for i in root: print(i.tag, i.attrib, i.text, i.tail, sep=';', end='') 也可以是根据某一个标签进行迭代 12345678for i in root.findall('country'): #遍历所有符合条件子节点...for i in root.find('country'): #遍历第一个符合条件子节点...for i in root.findtext('country'): #只遍历文本...for i in root.iter('country'): #以当前节点为树节点... 修改xml 需要注意的是，xml中所有字符均为字符串类型，需要注意字符转换 12345rank.text = str(new_rank) # 必须将int转为strrank.set("updated", "yes") # 添加属性del rank.attrib['updated'] #删除属性tree.write('111.xml') #将数据写入磁盘root.remove(country) #删除子节点 修改之后的内容只是放在内存中，所以需要将内存里面的数据保存到磁盘中 处理大型xml文件 当然，处理大型文档，除了使用固有的函数模块之外，还可以使用普通文档解析方式，这样只不过会导致取值更麻烦而已 其实只要一想到处理大型数据，就应该第一时间想到迭代器或者生成器 1234567891011121314151617181920212223from xml.etree.ElementTree import iterparsedef parse_and_remove(filename, path): path_parts = path.split('/') doc = iterparse(filename, ('start', 'end')) # Skip the root element next(doc) tag_stack = [] elem_stack = [] for event, elem in doc: if event == 'start': tag_stack.append(elem.tag) elem_stack.append(elem) elif event == 'end': if tag_stack == path_parts: yield elem elem_stack[-2].remove(elem) try: tag_stack.pop() elem_stack.pop() except IndexError: pass iterparse() 方法允许对XML文档进行增量操作。 使用时，你需要提供文件名和一个包含下面一种或多种类型的事件列表： start, end, start-ns 和 end-ns 。由 iterparse() 创建的迭代器会产生形如 (event, elem) 的元组， 其中 event 是上述事件列表中的某一个，而 elem 是相应的XML元素。 start 事件在某个元素第一次被创建并且还没有被插入其他数据(如子元素)时被创建。 而 end 事件在某个元素已经完成时被创建。 在 yield 之后的下面这个语句才是使得程序占用极少内存的ElementTree的核心特性： 1elem_stack[-2].remove(elem) 这个语句使得之前由 yield 产生的元素从它的父节点中删除掉。 假设已经没有其它的地方引用这个元素了，那么这个元素就被销毁并回收内存。 对节点的迭代式解析和删除的最终效果就是一个在文档上高效的增量式清扫过程。 文档树结构从始自终没被完整的创建过。尽管如此，还是能通过上述简单的方式来处理这个XML数据。 将字典类型数据转换为xml 存在两种解决方案： 手动构造，以字符串的format函数替代的方式来构造，不过这样显得有点蠢 使用xml.etree.ElementTree模块中的Element函数 123456789101112from xml.etree.ElementTree import Element, tostringdefdict_to_xml(tag, d): elem = Element(tag) for key, val in d.items(): child = Element(key) child.text = str(val) elem.append(child) return elems = &#123;'name': 'GOOG', 'shares': 100, 'price': 490.1&#125;e = dict_to_xml('stock', s) #&lt;Element 'stock' at 0x000001CE0548C908&gt;print(tostring(e).decode('utf-8'))#&lt;stock&gt;&lt;name&gt;GOOG&lt;/name&gt;&lt;shares&gt;100&lt;/shares&gt;&lt;price&gt;490.1&lt;/price&gt;&lt;/stock&gt; 这样做的目的在于，可以通过查询数据库中的值放进字典中，利用字典生成xml文件]]></content>
      <categories>
        <category>Python</category>
      </categories>
  </entry>
</search>
